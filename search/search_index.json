{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hello, my name is Lindsay I am a creative technologist based in Los Angeles, CA. I am passionate about implementing code to optimize artistic workflows, as well as leveraging design concepts to create intuitive and aesthetic interfaces for artists and developers alike. Education Columbia University ('19-'23) - Bachelor of Computer Science (GPA: 3.8) Certificates Stanford University ('24-'25) - Graduate Certificate in Visual Computing (GPA: 4.3) Resume Work Experience Walt Disney Animation Studios General Technical Director Trainee (2025-2026) General Technical Director Intern (2024) CLO Virtual Fashion Research & Development Intern (2023) Partizan Studio Animation Production Intern (2023) Leo Villareal Studio Web Graphics Developer (2022) MERGE Immersive Experience Design Intern (2022) Volunteer Work Columbia University Effective Altruism AI Research Fellow (2022) UCSF Benioff Childrens' Hospital Sports Medicine/PT Assistant (2015-2019) Press RatRock Magazine : \"Featured Artist\" United States Chess Federation : \"Top 50 Highest Rated Female Players\" USA Fencing \"All-American Team 2016-2017\" Lamorinda Weekly : \"Local Fencer Finishes 3rd at North American Cup\" \"Students at Berkeley Chess School Excel in State Competitions\": East Bay Times Interview","title":"Home"},{"location":"#hello-my-name-is-lindsay","text":"I am a creative technologist based in Los Angeles, CA. I am passionate about implementing code to optimize artistic workflows, as well as leveraging design concepts to create intuitive and aesthetic interfaces for artists and developers alike.","title":"Hello, my name is Lindsay"},{"location":"#education","text":"Columbia University ('19-'23) - Bachelor of Computer Science (GPA: 3.8)","title":"Education"},{"location":"#certificates","text":"Stanford University ('24-'25) - Graduate Certificate in Visual Computing (GPA: 4.3)","title":"Certificates"},{"location":"#resume","text":"","title":"Resume"},{"location":"#work-experience","text":"Walt Disney Animation Studios General Technical Director Trainee (2025-2026) General Technical Director Intern (2024) CLO Virtual Fashion Research & Development Intern (2023) Partizan Studio Animation Production Intern (2023) Leo Villareal Studio Web Graphics Developer (2022) MERGE Immersive Experience Design Intern (2022)","title":"Work Experience"},{"location":"#volunteer-work","text":"Columbia University Effective Altruism AI Research Fellow (2022) UCSF Benioff Childrens' Hospital Sports Medicine/PT Assistant (2015-2019)","title":"Volunteer Work"},{"location":"#press","text":"RatRock Magazine : \"Featured Artist\" United States Chess Federation : \"Top 50 Highest Rated Female Players\" USA Fencing \"All-American Team 2016-2017\" Lamorinda Weekly : \"Local Fencer Finishes 3rd at North American Cup\" \"Students at Berkeley Chess School Excel in State Competitions\": East Bay Times Interview","title":"Press"},{"location":"project0/","text":"Web-Based IK Rig & Keyframe Editor: Scene Controls Orbit controls are implemented for easy control and maneuvering around this 3D scene. Both the scene and controls are centered about the origin. Cmd Ctrl Zoom in Scroll forward Zoom out Scroll backward Tilt Drag up/down Pan Drag left/right GUI Control Details Show IK Toggle on/off to show/hide the mesh skeleton Show Transform Toggle on/off to show/hide the target object for the rig Parent Higher of the two bones in the selected joint in the bone hierarchy Child Lower of the two bones in the selected joint in the bone hierarchy Rotation min/max Min and Max amount (in radians) the selected joint is allowed to rotate Keyframe Editor This project provides a JavaScript-based keyframing tool for animating 3D models. The tool allows users to set keyframes at specific times, enabling the creation of smooth animations over time. Function Button Add Keyframe Remove Keyframe Start Playback Pause Playback Scrub/Step Project Overview The project offers technical artists a streamlined toolset for rigging and animating 3D models efficiently. By leveraging inverse kinematics, keyframe animation tools, a user-friendly GUI, and interactive controls, it provides a simple and intuitive environment for creating and manipulating skeletal structures and animations within a web-based 3D graphics application. Core Functionalities 1. Rendering and Animation Loop: Manages rendering with a timestamp and frame number, incorporating the IK solver and timeline for playback control. function render(timestamp, frame) { IKSolver?.update(); const delta = clock.getDelta(); if(playing){ timeline.setTime(start*1000 + (1000*(clock.elapsedTime + delta) % (loopduration*1000))); } if(mixer){ mixer.update(delta); } renderer.render(scene, camera); } 2. 3D Models: All 3D Models were custom built for this project with Blender imac.glb succulent.glb Imports 3D models using GLTFLoader. Models were imported as .glb files 4. Inverse Kinematics (IK) Implementation: Algorithm: A Cyclic Coordinate Descent (CCD) Solver is an algorithm used in 3D graphics and simulations, particularly in rigging for animation and robotics. It's employed in solving the inverse kinematics problem in multi-joint chains. It is ideal for real-time/web-based applications due to its efficiency. Functionality : The CCD Solver iteratively adjusts joint angles within a chain-like structure, altering one joint at a time to achieve a desired position or orientation for the end-effector. Process : It starts from the end-effector and iterates backward, adjusting each joint's angles in a cyclic manner. The adjustments aim to align the end-effector with the target position or orientation. Efficiency : CCD is computationally efficient and well-suited for systems with interconnected joints. It often converges rapidly toward the target, especially in systems where simple, iterative solutions are effective. THREE.js CCDIK Implementation: Utilizes a CCD IK solver for manipulating skeletal structures. const iks = [ { target: 5, // \"target\" effector: 4, // \"bone3\" links: [ { index: 3 }, { index: 2 }, { index: 1 } ] // \"bone2\", \"bone1\", \"bone0\" } ]; Constructs bone hierarchies // Bones hierarchy: // // root // \u251c\u2500\u2500 bone0 // \u2502 \u2514\u2500\u2500 bone1 // \u2502 \u2514\u2500\u2500 bone2 // \u2502 \u2514\u2500\u2500 bone3 // \u2514\u2500\u2500 target Allows position transformations of IK target object // Positioned as follow on the cylinder: // // o <- target (y = 20) // // +----o----+ <- bone3 (y = 12) // | | // | o | <- bone2 (y = 4) // | | // | o | <- bone1 (y = -4) // | | // +----oo---+ <- root, bone0 (y = -12) Joint Constraint : IK calculations with unlimited joint rotation are unstable and difficult for 3D animators to control The solver will often produce undesirable effects when pushed to extremes of possible joint configurations: Loose Constraints Tight Constraints Solution: Custom Flexibility: Setting a limit on the range of motion of each joint increases the overall stability of the IK rig GUI panel contains joint-wise rotation constraint adjustments. Parent child relationships are exposed in the menu as well Keyframe Editor Implementation Functionality The tool uses the animation-js javascript package to manage keyframes and timeline interactions. Keyframes are stored as time-value pairs, representing the position of the 3D model at specific points in time. Playback controls manage the animation flow, utilizing the provided clock to control timing. Key Components Timeline interface that displays keyframes and manages animation time. let positionKF = new VectorKeyframeTrack( \".position\", times, values ) let rows = [ { keyframes: [ { val: 0, }, ] } ]; timeline = new timelineModule.Timeline({id:'timeline'}) timeline.setModel({ rows: rows }); Toolbar Functions The code is structured into functions for: setUpAnimation() : Initialize keyframe tracks and turns IK target position into keyframable attribute function setupAnimation(){ const positionKF = new VectorKeyframeTrack( \".position\", times, values ) const tracks = [positionKF]; const length = -1; const clip = new AnimationClip(\"movetarget\",length,tracks); mixer = new AnimationMixer(handleBone); action = mixer.clipAction(clip); } addKeyframe() : Adding keyframes based on the current time and model position. function addKeyframe() { if (timeline) { const currentModel = timeline.getModel(); currentModel.rows[0].keyframes.push({ val: timeline.getTime() }); timeline.setModel(currentModel); values.push(handleBone.position.x, handleBone.position.y, handleBone.position.z); times.push(timeline.getTime()/1000); // if loop is affected, update the start, end and duration start = Math.min.apply(Math, times); end = Math.max.apply(Math, times); loopduration = end - start; // replace old keyframe track with updated one tracks = [new VectorKeyframeTrack( \".position\",times,values)]; length = -1; } } removeKeyframe() : Removing selected keyframes from the timeline. function removeKeyframe() { // delete values and times entry let temp = []; let idx; for(const i in times){ //console.log(i,t); if(times[i]!=t){ temp.push(Number(times[i])); } else{ // save index so we know which values to remove idx = i; } } times = temp; console.log(\"after:\", times); let temp2 = []; for(const j in values){ // x y and z of position vector at index i (each index has width 3) if(j != 3*idx){ if(j!= 3*idx+1){ if(j!= 3*idx +2){ temp2.push(Number(values[j])); } } } } } onPlayClick() / onPauseClick() : Handling play, pause, and time update events. function onPlayClick(event) { action.play(); clock.start(); playing = true; start = Math.min.apply(Math, times); end = Math.max.apply(Math, times); loopduration = end - start; if (timeline) { // Don't allow to manipulate timeline during playing (optional). timeline.setOptions({ timelineDraggable: false }); } } interpolate() : Interpolates between keyframes to create smooth animation transitions: function interpolate(start, end, t){ const x = start.x + (end.x - start.x) * t; const y = start.y + (end.y - start.y) * t; const z = start.z + (end.z - start.z) * t; return new THREE.Vector3(x, y, z); } timeline.onTimeChanged() : For scrubbing/stepping through paused timeline: onTimeChanged(){ time = timeline.getTime(); min = timeline.values.getMax(); max = timeline.values.getMin(); // minmax problem: // max = maximum value less than time // min = minimum value greater than time if(time < min && time > max){ let minidx, maxidx; for(const k in kf){ let currval =kf[k].val; if(currval <= min && currval > time){ // upper bound min = kf[k].val; minidx = k; } if(currval >= max && currval < time){ // lower bound max = kf[k].val; maxidx = k; } } const v1 = new THREE.Vector3(values[3*minidx],values[3*minidx+1],values[3*minidx+2]); const v2 = new THREE.Vector3(values[3*maxidx],values[3*maxidx+1],values[3*maxidx+2]); const t = (time-max)/(min-max); currPos = interpolate(v2, v1,t); handleBone.position.x = currPos.x; handleBone.position.y = currPos.y; handleBone.position.z = currPos.z; } } The tool offers an intuitive interface Technologies Used WebGL : Renders 3D graphics in the browser. Three.js : JavaScript library for 3D graphics. GLTF : Efficient file format for 3D scenes and models. dat.gui : interactive GUI controls animation-timeline-js : Javascript package for users to create and manage keyframed animations for 3D models. Conclusion This project offers technical artists a robust toolset for rigging and animating 3D models. Leveraging inverse kinematics, an intuitive GUI, and interactive controls, it provides a comprehensive environment for creating and manipulating skeletal structures and animations within a web-based 3D graphics application.","title":"Web-Based IK Rig & Keyframe Editor"},{"location":"project0/#web-based-ik-rig-keyframe-editor","text":"","title":"Web-Based IK Rig &amp; Keyframe Editor:"},{"location":"project0/#scene-controls","text":"Orbit controls are implemented for easy control and maneuvering around this 3D scene. Both the scene and controls are centered about the origin. Cmd Ctrl Zoom in Scroll forward Zoom out Scroll backward Tilt Drag up/down Pan Drag left/right","title":"Scene Controls"},{"location":"project0/#gui","text":"Control Details Show IK Toggle on/off to show/hide the mesh skeleton Show Transform Toggle on/off to show/hide the target object for the rig Parent Higher of the two bones in the selected joint in the bone hierarchy Child Lower of the two bones in the selected joint in the bone hierarchy Rotation min/max Min and Max amount (in radians) the selected joint is allowed to rotate","title":"GUI"},{"location":"project0/#keyframe-editor","text":"This project provides a JavaScript-based keyframing tool for animating 3D models. The tool allows users to set keyframes at specific times, enabling the creation of smooth animations over time. Function Button Add Keyframe Remove Keyframe Start Playback Pause Playback Scrub/Step","title":"Keyframe Editor"},{"location":"project0/#project-overview","text":"The project offers technical artists a streamlined toolset for rigging and animating 3D models efficiently. By leveraging inverse kinematics, keyframe animation tools, a user-friendly GUI, and interactive controls, it provides a simple and intuitive environment for creating and manipulating skeletal structures and animations within a web-based 3D graphics application.","title":"Project Overview"},{"location":"project0/#core-functionalities","text":"","title":"Core Functionalities"},{"location":"project0/#1-rendering-and-animation-loop","text":"Manages rendering with a timestamp and frame number, incorporating the IK solver and timeline for playback control. function render(timestamp, frame) { IKSolver?.update(); const delta = clock.getDelta(); if(playing){ timeline.setTime(start*1000 + (1000*(clock.elapsedTime + delta) % (loopduration*1000))); } if(mixer){ mixer.update(delta); } renderer.render(scene, camera); }","title":"1. Rendering and Animation Loop:"},{"location":"project0/#2-3d-models","text":"All 3D Models were custom built for this project with Blender imac.glb succulent.glb Imports 3D models using GLTFLoader. Models were imported as .glb files","title":"2. 3D Models:"},{"location":"project0/#4-inverse-kinematics-ik-implementation","text":"","title":"4. Inverse Kinematics (IK) Implementation:"},{"location":"project0/#algorithm","text":"A Cyclic Coordinate Descent (CCD) Solver is an algorithm used in 3D graphics and simulations, particularly in rigging for animation and robotics. It's employed in solving the inverse kinematics problem in multi-joint chains. It is ideal for real-time/web-based applications due to its efficiency. Functionality : The CCD Solver iteratively adjusts joint angles within a chain-like structure, altering one joint at a time to achieve a desired position or orientation for the end-effector. Process : It starts from the end-effector and iterates backward, adjusting each joint's angles in a cyclic manner. The adjustments aim to align the end-effector with the target position or orientation. Efficiency : CCD is computationally efficient and well-suited for systems with interconnected joints. It often converges rapidly toward the target, especially in systems where simple, iterative solutions are effective. THREE.js CCDIK Implementation: Utilizes a CCD IK solver for manipulating skeletal structures. const iks = [ { target: 5, // \"target\" effector: 4, // \"bone3\" links: [ { index: 3 }, { index: 2 }, { index: 1 } ] // \"bone2\", \"bone1\", \"bone0\" } ]; Constructs bone hierarchies // Bones hierarchy: // // root // \u251c\u2500\u2500 bone0 // \u2502 \u2514\u2500\u2500 bone1 // \u2502 \u2514\u2500\u2500 bone2 // \u2502 \u2514\u2500\u2500 bone3 // \u2514\u2500\u2500 target Allows position transformations of IK target object // Positioned as follow on the cylinder: // // o <- target (y = 20) // // +----o----+ <- bone3 (y = 12) // | | // | o | <- bone2 (y = 4) // | | // | o | <- bone1 (y = -4) // | | // +----oo---+ <- root, bone0 (y = -12)","title":"Algorithm:"},{"location":"project0/#joint-constraint","text":"IK calculations with unlimited joint rotation are unstable and difficult for 3D animators to control The solver will often produce undesirable effects when pushed to extremes of possible joint configurations: Loose Constraints Tight Constraints","title":"Joint Constraint :"},{"location":"project0/#solution-custom-flexibility","text":"Setting a limit on the range of motion of each joint increases the overall stability of the IK rig GUI panel contains joint-wise rotation constraint adjustments. Parent child relationships are exposed in the menu as well","title":"Solution: Custom Flexibility:"},{"location":"project0/#keyframe-editor-implementation","text":"","title":"Keyframe Editor Implementation"},{"location":"project0/#functionality","text":"The tool uses the animation-js javascript package to manage keyframes and timeline interactions. Keyframes are stored as time-value pairs, representing the position of the 3D model at specific points in time. Playback controls manage the animation flow, utilizing the provided clock to control timing.","title":"Functionality"},{"location":"project0/#key-components","text":"","title":"Key Components"},{"location":"project0/#timeline","text":"interface that displays keyframes and manages animation time. let positionKF = new VectorKeyframeTrack( \".position\", times, values ) let rows = [ { keyframes: [ { val: 0, }, ] } ]; timeline = new timelineModule.Timeline({id:'timeline'}) timeline.setModel({ rows: rows });","title":"Timeline"},{"location":"project0/#toolbar-functions","text":"The code is structured into functions for: setUpAnimation() : Initialize keyframe tracks and turns IK target position into keyframable attribute function setupAnimation(){ const positionKF = new VectorKeyframeTrack( \".position\", times, values ) const tracks = [positionKF]; const length = -1; const clip = new AnimationClip(\"movetarget\",length,tracks); mixer = new AnimationMixer(handleBone); action = mixer.clipAction(clip); } addKeyframe() : Adding keyframes based on the current time and model position. function addKeyframe() { if (timeline) { const currentModel = timeline.getModel(); currentModel.rows[0].keyframes.push({ val: timeline.getTime() }); timeline.setModel(currentModel); values.push(handleBone.position.x, handleBone.position.y, handleBone.position.z); times.push(timeline.getTime()/1000); // if loop is affected, update the start, end and duration start = Math.min.apply(Math, times); end = Math.max.apply(Math, times); loopduration = end - start; // replace old keyframe track with updated one tracks = [new VectorKeyframeTrack( \".position\",times,values)]; length = -1; } } removeKeyframe() : Removing selected keyframes from the timeline. function removeKeyframe() { // delete values and times entry let temp = []; let idx; for(const i in times){ //console.log(i,t); if(times[i]!=t){ temp.push(Number(times[i])); } else{ // save index so we know which values to remove idx = i; } } times = temp; console.log(\"after:\", times); let temp2 = []; for(const j in values){ // x y and z of position vector at index i (each index has width 3) if(j != 3*idx){ if(j!= 3*idx+1){ if(j!= 3*idx +2){ temp2.push(Number(values[j])); } } } } } onPlayClick() / onPauseClick() : Handling play, pause, and time update events. function onPlayClick(event) { action.play(); clock.start(); playing = true; start = Math.min.apply(Math, times); end = Math.max.apply(Math, times); loopduration = end - start; if (timeline) { // Don't allow to manipulate timeline during playing (optional). timeline.setOptions({ timelineDraggable: false }); } } interpolate() : Interpolates between keyframes to create smooth animation transitions: function interpolate(start, end, t){ const x = start.x + (end.x - start.x) * t; const y = start.y + (end.y - start.y) * t; const z = start.z + (end.z - start.z) * t; return new THREE.Vector3(x, y, z); } timeline.onTimeChanged() : For scrubbing/stepping through paused timeline: onTimeChanged(){ time = timeline.getTime(); min = timeline.values.getMax(); max = timeline.values.getMin(); // minmax problem: // max = maximum value less than time // min = minimum value greater than time if(time < min && time > max){ let minidx, maxidx; for(const k in kf){ let currval =kf[k].val; if(currval <= min && currval > time){ // upper bound min = kf[k].val; minidx = k; } if(currval >= max && currval < time){ // lower bound max = kf[k].val; maxidx = k; } } const v1 = new THREE.Vector3(values[3*minidx],values[3*minidx+1],values[3*minidx+2]); const v2 = new THREE.Vector3(values[3*maxidx],values[3*maxidx+1],values[3*maxidx+2]); const t = (time-max)/(min-max); currPos = interpolate(v2, v1,t); handleBone.position.x = currPos.x; handleBone.position.y = currPos.y; handleBone.position.z = currPos.z; } } The tool offers an intuitive interface","title":"Toolbar Functions"},{"location":"project0/#technologies-used","text":"WebGL : Renders 3D graphics in the browser. Three.js : JavaScript library for 3D graphics. GLTF : Efficient file format for 3D scenes and models. dat.gui : interactive GUI controls animation-timeline-js : Javascript package for users to create and manage keyframed animations for 3D models.","title":"Technologies Used"},{"location":"project0/#conclusion","text":"This project offers technical artists a robust toolset for rigging and animating 3D models. Leveraging inverse kinematics, an intuitive GUI, and interactive controls, it provides a comprehensive environment for creating and manipulating skeletal structures and animations within a web-based 3D graphics application.","title":"Conclusion"},{"location":"project1/","text":"Interactive 3D Lite-Brite Display with Customizable Lighting Project Description: This technical project involves creating an interactive 3D scene that recreates a classic children's toy and a customizable lighting setup using the Three.js library. The scene consists of a 3D Mickey Mouse lightbox and a desk-like structure, with various elements that can be controlled and customized. Key Components and Features: 3D Scene Setup: import * as THREE from 'three'; const scene = new THREE.Scene(); const camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 0.1, 10000 ); const renderer = new THREE.WebGLRenderer(); renderer.shadowMap.enabled = true; renderer.shadowMap.type = THREE.PCFSoftShadowMap; // default THREE.PCFShadowMap The project utilizes the Three.js library to create a 3D scene. A camera is set up to view the scene from a specific perspective. A renderer is used to display the 3D scene in a web browser. Soft shadows are enabled for more realistic lighting Customizable Lighting: // LED panel const rectLight = new THREE.RectAreaLight( 0xF6E7D2, settings.brightness, width, height, ); // point light 1 const pt1 = new THREE.PointLight( 0xCBC3E3, 10, 500 ); pt1.position.set( 30, 50, 5 ); pt1.lookAt(12,20,0); // point light 2 const pt2 = new THREE.PointLight( 0xCBC3E3, 10, 1000 ); pt2.position.set( 0, 50, 5 ); pt2.lookAt(12,20,0); // point light 3 const pt3 = new THREE.PointLight(0xCBC3E3, 10, 500); pt3.position.set( 60, 50, 5 ); pt3.lookAt(12,20,0); // point light 4 const pt4 = new THREE.DirectionalLight( 0xCBC3E3, 1 ); pt4.position.set( 25,100,-70 ); pt4.rotation.y -= Math.PI/2; // ambient scene lighting const light = new THREE.AmbientLight( 0xF6E7D2 ); // soft white light light.position.z = -25; light.position.y = 5; light.intensity = 0; // add all lights to scene (obs) obs.add( rectLight, pt1, pt2, pt3, pt4, light ); The scene features customizable lighting using various types of light sources. Users can adjust the brightness of the scene's lighting using a GUI control. Lighting options include ambient light and multiple point lights with adjustable colors. Interactive Controls: import { OrbitControls } from 'three/addons/controls/OrbitControls.js'; import {GUI} from 'dat.gui'; const controls = new OrbitControls( camera, renderer.domElement ); controls.update(); const gui = new GUI(); const scenelights = gui.addFolder('Scene Lighting'); scenelights.add(settings, 'ambient', 0,1); scenelights.add(settings, 'point1'); scenelights.add(settings, 'point2'); scenelights.add(settings, 'point3'); The scene allows users to interact with the 3D objects using an OrbitControls interface. Users can rotate and zoom the camera to explore the scene. A GUI interface is provided to adjust the lighting settings and control the visibility of specific point lights. Material Customization: const lightsettings = gui.addFolder('Brite-Lite Settings'); lightsettings.addColor(settings, 'bulb1'); lightsettings.addColor(settings, 'bulb2'); lightsettings.addColor(settings, 'bulb3'); lightsettings.addColor(settings, 'bulb4'); lightsettings.addColor(settings, 'bulb5'); lightsettings.add(settings, 'brightness', 1, 500); The Mickey Mouse design is made up of different colored bulbs, and the project allows users to customize the colors of these materials using the GUI interface. Each bulb material can be individually adjusted in terms of color. 3D Shape Extrusion: const flmesh = new THREE.Mesh(fl, wallmat); // adjust rotation flmesh.rotation.z += Math.PI/2; flmesh.rotation.x -= Math.PI/2; // set position to centered and hieght just below y=0 flmesh.position.x = 30; flmesh.position.y = -.75; flmesh.position.z = 25; // allow desk to recieve shadows from objects in real time flmesh.receiveShadow = true; scene.add(flmesh); The project utilizes extrusion to create a desk-like structure within the scene. The desk-like structure is customizable in terms of color and material. Textures: // reflection mapping on lightbulbs const hdrEquirect = new RGBELoader().load( \"./src/empty_warehouse_01_2k.hdr\", () => { hdrEquirect.mapping = THREE.EquirectangularReflectionMapping; } ); Textures are applied to specific objects within the scene, enhancing their visual appearance. Realistic reflection on bulbs is achieved by mapping a HDR environment map of a warehouse with bright overhead lighting onto each sphere External Data Integration: fetch('src/Mickey Mouse - Sheet1.csv') .then(response =>response.text()) .then(text => { data = Papa.parse(text).data; for (var i = 0; i < data.length; i++) { mickey.push((int)(data[i])); } mickey = mickey.reverse(); }); Colors were translated manually from reference image, converted to 35 x 32 px array of pixel coordinates, and then converted to CSV file The project loads external data from a CSV file and incorporates it into the Mickey Mouse figure. Plotting Points on the Pegboard for(var i = h/8-2; i>=0; i--){ // for each row for(var j = 0; j<((w/8-1)+(1-i%2)); j++){ // for each column const newmesh = new THREE.Mesh(geometry1, mtls[mickey[ct]]); newmesh.scale.x = 0.6; newmesh.scale.y = 0.6; newmesh.scale.z = 0.6; if(i%2==0){ newmesh.position.x = 1.6*j+1.25; } else{ newmesh.position.x = 1.6*j+1.8; } newmesh.position.y = 1.6*i+1.8; newmesh.position.z = .5; obs.add(newmesh); if(i%2==0){ boardshape.holes.push(new THREE.EllipseCurve( ax+5, ay+8, // ax, aY 2.5, 2.5, // xRadius, yRadius 0, 2 * Math.PI, // aStartAngle, aEndAngle )); } else{ boardshape.holes.push(new THREE.EllipseCurve( ax+8, ay+8, // ax, aY 2.5, 2.5, // xRadius, yRadius 0, 2 * Math.PI, // aStartAngle, aEndAngle )); } ax = ax+8; ct++; } ax = 0; ay+=8; } The board in the reference image contained alternating rows of 35 and 36 holes, offset by one This loop has 2 functions Punch a circular hole into the box geometry of the pegboard, so the LED light can shine through Create a new instance of a lightbulb object of the appropriate color in each hole Performance Monitoring: import Stats from 'three/examples/jsm/libs/stats.module'; const stats = new Stats(); document.body.appendChild(stats.dom); The project incorporates a performance monitoring tool (Stats) to track and display the frame rate and other performance metrics. Responsive Design: renderer.setSize( window.innerWidth, window.innerHeight ); document.body.appendChild( renderer.domElement ); const width = 60; const height = 50; The scene adjusts to the dimensions of the user's browser window. This project offers an interactive and visually appealing 3D scene with Mickey Mouse and allows users to customize the lighting, materials, and more using a user-friendly graphical interface. Users can explore the scene from different angles and observe the visual effects of their customization choices. Animation Loop function animate() { requestAnimationFrame( animate ); rectLight.intensity = settings.brightness; light.intensity = settings.ambient; pt1.visible = settings.point1; pt2.visible= settings.point2; pt3.visible = settings.point3; material1.color.set(settings.bulb5); material2.color.set(settings.bulb1); material3.color.set(settings.bulb2); material4.color.set(settings.bulb3); material5.color.set(settings.bulb4); controls.update(); renderer.render( scene, camera ); stats.update(); } The animation loop is constantly updated to reflect changes in the GUI parameters as well as any window size changes","title":"Interactive Lite-Brite Toy"},{"location":"project1/#interactive-3d-lite-brite-display-with-customizable-lighting","text":"","title":"Interactive 3D Lite-Brite Display with Customizable Lighting"},{"location":"project1/#project-description","text":"This technical project involves creating an interactive 3D scene that recreates a classic children's toy and a customizable lighting setup using the Three.js library. The scene consists of a 3D Mickey Mouse lightbox and a desk-like structure, with various elements that can be controlled and customized.","title":"Project Description:"},{"location":"project1/#key-components-and-features","text":"","title":"Key Components and Features:"},{"location":"project1/#3d-scene-setup","text":"import * as THREE from 'three'; const scene = new THREE.Scene(); const camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 0.1, 10000 ); const renderer = new THREE.WebGLRenderer(); renderer.shadowMap.enabled = true; renderer.shadowMap.type = THREE.PCFSoftShadowMap; // default THREE.PCFShadowMap The project utilizes the Three.js library to create a 3D scene. A camera is set up to view the scene from a specific perspective. A renderer is used to display the 3D scene in a web browser. Soft shadows are enabled for more realistic lighting","title":"3D Scene Setup:"},{"location":"project1/#customizable-lighting","text":"// LED panel const rectLight = new THREE.RectAreaLight( 0xF6E7D2, settings.brightness, width, height, ); // point light 1 const pt1 = new THREE.PointLight( 0xCBC3E3, 10, 500 ); pt1.position.set( 30, 50, 5 ); pt1.lookAt(12,20,0); // point light 2 const pt2 = new THREE.PointLight( 0xCBC3E3, 10, 1000 ); pt2.position.set( 0, 50, 5 ); pt2.lookAt(12,20,0); // point light 3 const pt3 = new THREE.PointLight(0xCBC3E3, 10, 500); pt3.position.set( 60, 50, 5 ); pt3.lookAt(12,20,0); // point light 4 const pt4 = new THREE.DirectionalLight( 0xCBC3E3, 1 ); pt4.position.set( 25,100,-70 ); pt4.rotation.y -= Math.PI/2; // ambient scene lighting const light = new THREE.AmbientLight( 0xF6E7D2 ); // soft white light light.position.z = -25; light.position.y = 5; light.intensity = 0; // add all lights to scene (obs) obs.add( rectLight, pt1, pt2, pt3, pt4, light ); The scene features customizable lighting using various types of light sources. Users can adjust the brightness of the scene's lighting using a GUI control. Lighting options include ambient light and multiple point lights with adjustable colors.","title":"Customizable Lighting:"},{"location":"project1/#interactive-controls","text":"import { OrbitControls } from 'three/addons/controls/OrbitControls.js'; import {GUI} from 'dat.gui'; const controls = new OrbitControls( camera, renderer.domElement ); controls.update(); const gui = new GUI(); const scenelights = gui.addFolder('Scene Lighting'); scenelights.add(settings, 'ambient', 0,1); scenelights.add(settings, 'point1'); scenelights.add(settings, 'point2'); scenelights.add(settings, 'point3'); The scene allows users to interact with the 3D objects using an OrbitControls interface. Users can rotate and zoom the camera to explore the scene. A GUI interface is provided to adjust the lighting settings and control the visibility of specific point lights.","title":"Interactive Controls:"},{"location":"project1/#material-customization","text":"const lightsettings = gui.addFolder('Brite-Lite Settings'); lightsettings.addColor(settings, 'bulb1'); lightsettings.addColor(settings, 'bulb2'); lightsettings.addColor(settings, 'bulb3'); lightsettings.addColor(settings, 'bulb4'); lightsettings.addColor(settings, 'bulb5'); lightsettings.add(settings, 'brightness', 1, 500); The Mickey Mouse design is made up of different colored bulbs, and the project allows users to customize the colors of these materials using the GUI interface. Each bulb material can be individually adjusted in terms of color.","title":"Material Customization:"},{"location":"project1/#3d-shape-extrusion","text":"const flmesh = new THREE.Mesh(fl, wallmat); // adjust rotation flmesh.rotation.z += Math.PI/2; flmesh.rotation.x -= Math.PI/2; // set position to centered and hieght just below y=0 flmesh.position.x = 30; flmesh.position.y = -.75; flmesh.position.z = 25; // allow desk to recieve shadows from objects in real time flmesh.receiveShadow = true; scene.add(flmesh); The project utilizes extrusion to create a desk-like structure within the scene. The desk-like structure is customizable in terms of color and material.","title":"3D Shape Extrusion:"},{"location":"project1/#textures","text":"// reflection mapping on lightbulbs const hdrEquirect = new RGBELoader().load( \"./src/empty_warehouse_01_2k.hdr\", () => { hdrEquirect.mapping = THREE.EquirectangularReflectionMapping; } ); Textures are applied to specific objects within the scene, enhancing their visual appearance. Realistic reflection on bulbs is achieved by mapping a HDR environment map of a warehouse with bright overhead lighting onto each sphere","title":"Textures:"},{"location":"project1/#external-data-integration","text":"fetch('src/Mickey Mouse - Sheet1.csv') .then(response =>response.text()) .then(text => { data = Papa.parse(text).data; for (var i = 0; i < data.length; i++) { mickey.push((int)(data[i])); } mickey = mickey.reverse(); }); Colors were translated manually from reference image, converted to 35 x 32 px array of pixel coordinates, and then converted to CSV file The project loads external data from a CSV file and incorporates it into the Mickey Mouse figure.","title":"External Data Integration:"},{"location":"project1/#plotting-points-on-the-pegboard","text":"for(var i = h/8-2; i>=0; i--){ // for each row for(var j = 0; j<((w/8-1)+(1-i%2)); j++){ // for each column const newmesh = new THREE.Mesh(geometry1, mtls[mickey[ct]]); newmesh.scale.x = 0.6; newmesh.scale.y = 0.6; newmesh.scale.z = 0.6; if(i%2==0){ newmesh.position.x = 1.6*j+1.25; } else{ newmesh.position.x = 1.6*j+1.8; } newmesh.position.y = 1.6*i+1.8; newmesh.position.z = .5; obs.add(newmesh); if(i%2==0){ boardshape.holes.push(new THREE.EllipseCurve( ax+5, ay+8, // ax, aY 2.5, 2.5, // xRadius, yRadius 0, 2 * Math.PI, // aStartAngle, aEndAngle )); } else{ boardshape.holes.push(new THREE.EllipseCurve( ax+8, ay+8, // ax, aY 2.5, 2.5, // xRadius, yRadius 0, 2 * Math.PI, // aStartAngle, aEndAngle )); } ax = ax+8; ct++; } ax = 0; ay+=8; } The board in the reference image contained alternating rows of 35 and 36 holes, offset by one This loop has 2 functions Punch a circular hole into the box geometry of the pegboard, so the LED light can shine through Create a new instance of a lightbulb object of the appropriate color in each hole","title":"Plotting Points on the Pegboard"},{"location":"project1/#performance-monitoring","text":"import Stats from 'three/examples/jsm/libs/stats.module'; const stats = new Stats(); document.body.appendChild(stats.dom); The project incorporates a performance monitoring tool (Stats) to track and display the frame rate and other performance metrics.","title":"Performance Monitoring:"},{"location":"project1/#responsive-design","text":"renderer.setSize( window.innerWidth, window.innerHeight ); document.body.appendChild( renderer.domElement ); const width = 60; const height = 50; The scene adjusts to the dimensions of the user's browser window. This project offers an interactive and visually appealing 3D scene with Mickey Mouse and allows users to customize the lighting, materials, and more using a user-friendly graphical interface. Users can explore the scene from different angles and observe the visual effects of their customization choices.","title":"Responsive Design:"},{"location":"project1/#animation-loop","text":"function animate() { requestAnimationFrame( animate ); rectLight.intensity = settings.brightness; light.intensity = settings.ambient; pt1.visible = settings.point1; pt2.visible= settings.point2; pt3.visible = settings.point3; material1.color.set(settings.bulb5); material2.color.set(settings.bulb1); material3.color.set(settings.bulb2); material4.color.set(settings.bulb3); material5.color.set(settings.bulb4); controls.update(); renderer.render( scene, camera ); stats.update(); } The animation loop is constantly updated to reflect changes in the GUI parameters as well as any window size changes","title":"Animation Loop"},{"location":"project2/","text":"WebGL Stereoscope Effect Overview The application sets up a 3D scene containing multiple spheres and allows users to switch between different rendering effects: Stereo, Anaglyph, and Parallax. It offers real-time interaction by capturing mouse movements to control the camera's position and direction. What is Stereoscopic Rendering? Stereoscopic rendering is based on the principle of presenting two slightly different images to each eye, mimicking the way human eyes perceive depth in the real world. It involves: Dual Perspective Rendering: Two separate views of the scene, representing the left-eye and right-eye perspectives, are generated. These views simulate the slight offset observed by each eye in a natural setting. Displaying Views: The left-eye and right-eye images are typically displayed side by side or in quick succession. When viewed through stereoscopic glasses (often polarized or active-shutter glasses), each eye perceives its respective image, resulting in a perception of depth and three-dimensionality. Adjusting Camera Settings: In 3D applications, the camera setup is crucial. The camera settings need to mimic human eye separation to create an accurate stereoscopic effect. Parallax Barrier The parallax barrier effect is a glasses-free 3D technology used in displays, such as autostereoscopic screens (screens that provide 3D without requiring glasses). Key aspects include: Barrier Layer: A barrier layer consisting of a series of precise slits or barriers is placed in front of the display panel. Directional Light Control: The barrier restricts the direction of light emitted by the display, ensuring different pixels or subpixels are visible from different viewing angles. Perspective Variation: Each eye sees a slightly different set of pixels due to the barrier's directional control, creating a stereoscopic effect without requiring glasses. Anaglyph Rendering Anaglyph rendering uses color filtering to create the illusion of depth perception without specialized displays. It involves: Dual-Color Images: Two images of the scene are rendered, each overlaid with a different color filter (typically red-cyan or red-blue). Use of Glasses: Anaglyph glasses (with corresponding color filters) are worn by the viewer. Each lens filters out one of the colored images, ensuring each eye sees only its respective view. Color Separation: When the filtered images are superimposed in the brain, the color differences create a perception of depth and three-dimensionality. Key Components 1. Scene Initialization ( init() Function) // create GUI to switch between fx const gui = new GUI() var stereofx = gui.addFolder('Stereo Camera'); gui.add(settings,'Effects',['Stereo','Anaglyph','Parallax']).onChange(function(value){ switchEffect(value); }); gui.open() var curreffect = settings.Effects; container = document.createElement( 'div' ); document.body.appendChild( container ); camera = new THREE.PerspectiveCamera( 60, window.innerWidth / window.innerHeight, 1, 100000 ); camera.position.z = 3200; scene = new THREE.Scene(); // Environment rendered as Cubemap scene.background = new THREE.CubeTextureLoader() .setPath( 'textures/cubemaps/' ) .load( [ 'px.png', 'nx.png', 'py.png', 'ny.png', 'pz.png', 'nz.png' ] ); GUI Creation: Utilizes dat.GUI to create an interface for selecting rendering effects. Environment Setup: Establishes the 3D environment, including a container for the scene, camera, textures, materials, and sphere meshes. Renderer Configuration: Configures the WebGL renderer and sets its pixel ratio. 2. Effect Switching ( switchEffect(value) Function) function switchEffect(value){ if(value == 'Stereo'){ effect = new StereoEffect( renderer ); effect.setSize( window.innerWidth, window.innerHeight ); } if(value == 'Anaglyph'){ effect = new AnaglyphEffect(renderer); effect.setSize(window.innerWidth, window.innerHeight ); } if(value == 'Parallax'){ effect = new ParallaxBarrierEffect(renderer); effect.setSize(window.innerWidth, window.innerHeight); } } Effect Handling: Dynamically switches the rendering effect based on user selection in the GUI (Stereo, Anaglyph, Parallax). 3. Event Handling function onWindowResize() { windowHalfX = window.innerWidth / 2; windowHalfY = window.innerHeight / 2; camera.aspect = window.innerWidth / window.innerHeight; camera.updateProjectionMatrix(); effect.setSize( window.innerWidth, window.innerHeight ); } Window Resize: Adjusts camera aspect ratio and effect size upon window resize. function onDocumentMouseMove( event ) { mouseX = ( event.clientX - windowHalfX ) * 10; mouseY = ( event.clientY - windowHalfY ) * 10; } Mouse Movement: Tracks mouse movement to control the camera's position and orientation. 4.Animation Loop function animate() { requestAnimationFrame( animate ); render(); } animate() Function: Starts the animation loop using requestAnimationFrame. function render() { const timer = 0.0001 * Date.now(); camera.position.x += ( mouseX - camera.position.x ) * .05; camera.position.y += ( - mouseY - camera.position.y ) * .05; camera.lookAt( scene.position ); for ( let i = 0, il = spheres.length; i < il; i ++ ) { const sphere = spheres[ i ]; sphere.position.x = 5000 * Math.cos( timer + i ); sphere.position.y = 5000 * Math.sin( timer + i * 1.1 ); } effect.render( scene, camera ); } render() Function: Updates the scene elements and camera positions based on mouse movement and time, then renders the selected effect. Breakdown of Operations GUI Setup: Enables users to choose between rendering effects. Scene Creation: Generates a 3D environment with spheres positioned randomly within the scene. Rendering Pipeline: Facilitates rendering through WebGL using different effects. User Interaction: Captures mouse movement to dynamically control the camera's viewpoint. Responsive Design: Ensures the application adapts to window size changes for a consistent user experience. Overall Functionality Interactive Visualization: Presents a 3D scene with the ability to switch between multiple rendering effects. Real-time Interaction: Allows users to manipulate the scene via mouse movement. Adaptive Rendering: Handles dynamic resizing of the viewport for consistent display across various window sizes.","title":"WebGL Stereoscope Effect"},{"location":"project2/#webgl-stereoscope-effect","text":"","title":"WebGL Stereoscope Effect"},{"location":"project2/#overview","text":"The application sets up a 3D scene containing multiple spheres and allows users to switch between different rendering effects: Stereo, Anaglyph, and Parallax. It offers real-time interaction by capturing mouse movements to control the camera's position and direction.","title":"Overview"},{"location":"project2/#what-is-stereoscopic-rendering","text":"Stereoscopic rendering is based on the principle of presenting two slightly different images to each eye, mimicking the way human eyes perceive depth in the real world. It involves: Dual Perspective Rendering: Two separate views of the scene, representing the left-eye and right-eye perspectives, are generated. These views simulate the slight offset observed by each eye in a natural setting. Displaying Views: The left-eye and right-eye images are typically displayed side by side or in quick succession. When viewed through stereoscopic glasses (often polarized or active-shutter glasses), each eye perceives its respective image, resulting in a perception of depth and three-dimensionality. Adjusting Camera Settings: In 3D applications, the camera setup is crucial. The camera settings need to mimic human eye separation to create an accurate stereoscopic effect.","title":"What is Stereoscopic Rendering?"},{"location":"project2/#parallax-barrier","text":"The parallax barrier effect is a glasses-free 3D technology used in displays, such as autostereoscopic screens (screens that provide 3D without requiring glasses). Key aspects include: Barrier Layer: A barrier layer consisting of a series of precise slits or barriers is placed in front of the display panel. Directional Light Control: The barrier restricts the direction of light emitted by the display, ensuring different pixels or subpixels are visible from different viewing angles. Perspective Variation: Each eye sees a slightly different set of pixels due to the barrier's directional control, creating a stereoscopic effect without requiring glasses.","title":"Parallax Barrier"},{"location":"project2/#anaglyph-rendering","text":"Anaglyph rendering uses color filtering to create the illusion of depth perception without specialized displays. It involves: Dual-Color Images: Two images of the scene are rendered, each overlaid with a different color filter (typically red-cyan or red-blue). Use of Glasses: Anaglyph glasses (with corresponding color filters) are worn by the viewer. Each lens filters out one of the colored images, ensuring each eye sees only its respective view. Color Separation: When the filtered images are superimposed in the brain, the color differences create a perception of depth and three-dimensionality.","title":"Anaglyph Rendering"},{"location":"project2/#key-components","text":"","title":"Key Components"},{"location":"project2/#1-scene-initialization-init-function","text":"// create GUI to switch between fx const gui = new GUI() var stereofx = gui.addFolder('Stereo Camera'); gui.add(settings,'Effects',['Stereo','Anaglyph','Parallax']).onChange(function(value){ switchEffect(value); }); gui.open() var curreffect = settings.Effects; container = document.createElement( 'div' ); document.body.appendChild( container ); camera = new THREE.PerspectiveCamera( 60, window.innerWidth / window.innerHeight, 1, 100000 ); camera.position.z = 3200; scene = new THREE.Scene(); // Environment rendered as Cubemap scene.background = new THREE.CubeTextureLoader() .setPath( 'textures/cubemaps/' ) .load( [ 'px.png', 'nx.png', 'py.png', 'ny.png', 'pz.png', 'nz.png' ] ); GUI Creation: Utilizes dat.GUI to create an interface for selecting rendering effects. Environment Setup: Establishes the 3D environment, including a container for the scene, camera, textures, materials, and sphere meshes. Renderer Configuration: Configures the WebGL renderer and sets its pixel ratio.","title":"1. Scene Initialization (init() Function)"},{"location":"project2/#2-effect-switching-switcheffectvalue-function","text":"function switchEffect(value){ if(value == 'Stereo'){ effect = new StereoEffect( renderer ); effect.setSize( window.innerWidth, window.innerHeight ); } if(value == 'Anaglyph'){ effect = new AnaglyphEffect(renderer); effect.setSize(window.innerWidth, window.innerHeight ); } if(value == 'Parallax'){ effect = new ParallaxBarrierEffect(renderer); effect.setSize(window.innerWidth, window.innerHeight); } } Effect Handling: Dynamically switches the rendering effect based on user selection in the GUI (Stereo, Anaglyph, Parallax).","title":"2. Effect Switching (switchEffect(value) Function)"},{"location":"project2/#3-event-handling","text":"function onWindowResize() { windowHalfX = window.innerWidth / 2; windowHalfY = window.innerHeight / 2; camera.aspect = window.innerWidth / window.innerHeight; camera.updateProjectionMatrix(); effect.setSize( window.innerWidth, window.innerHeight ); } Window Resize: Adjusts camera aspect ratio and effect size upon window resize. function onDocumentMouseMove( event ) { mouseX = ( event.clientX - windowHalfX ) * 10; mouseY = ( event.clientY - windowHalfY ) * 10; } Mouse Movement: Tracks mouse movement to control the camera's position and orientation.","title":"3. Event Handling"},{"location":"project2/#4animation-loop","text":"function animate() { requestAnimationFrame( animate ); render(); } animate() Function: Starts the animation loop using requestAnimationFrame. function render() { const timer = 0.0001 * Date.now(); camera.position.x += ( mouseX - camera.position.x ) * .05; camera.position.y += ( - mouseY - camera.position.y ) * .05; camera.lookAt( scene.position ); for ( let i = 0, il = spheres.length; i < il; i ++ ) { const sphere = spheres[ i ]; sphere.position.x = 5000 * Math.cos( timer + i ); sphere.position.y = 5000 * Math.sin( timer + i * 1.1 ); } effect.render( scene, camera ); } render() Function: Updates the scene elements and camera positions based on mouse movement and time, then renders the selected effect.","title":"4.Animation Loop"},{"location":"project2/#breakdown-of-operations","text":"GUI Setup: Enables users to choose between rendering effects. Scene Creation: Generates a 3D environment with spheres positioned randomly within the scene. Rendering Pipeline: Facilitates rendering through WebGL using different effects. User Interaction: Captures mouse movement to dynamically control the camera's viewpoint. Responsive Design: Ensures the application adapts to window size changes for a consistent user experience.","title":"Breakdown of Operations"},{"location":"project2/#overall-functionality","text":"Interactive Visualization: Presents a 3D scene with the ability to switch between multiple rendering effects. Real-time Interaction: Allows users to manipulate the scene via mouse movement. Adaptive Rendering: Handles dynamic resizing of the viewport for consistent display across various window sizes.","title":"Overall Functionality"},{"location":"project3/","text":"Node Graph Visualizer Node Graph Visualizer 1. Introduction The Node Graph Visualizer is a graphical user interface that visually displays relationships between nodes in a Dependency Graph software architecture The Dependency Graph (aka \"Node Graph\") structure is the industry standard for developing graphics applications The corresponding Visualizer is simply a user-friendly and code-free interface that can be used to manipulate nodes in real-time rendering applications . Maya , ZBrush , Houdini , Blender , Unity , Cinema 4D , and most other 3D Content Creation tools use a Dependency Graph for their codebase due its efficiency, flexibility, and compatibility with graphics applications. These softwares also include a code-free interface for technical users to create custom nodes and plug-ins. For example, Autodesk Maya's Hypershade was the blueprint for this visualizer project. Hypershade 2. Goals and Objectives The UI design goals were: Create an intuitive and aesthetic interface for developers Streamline workflows and improve productivity Enable easy integration with existing C++ codebase Implementation Objectives: Smart Default Layout Draggable Nodes Pin Objects Instantiate new nodes Draw Connections Delete Nodes/Connections Edit Plug Attributes View Plug Attributes Key Features Viewing Modes Simple Displays only one input plug and output plug Remaining plugs are hidden Connected Displays only the input/output plugs that are actively connected Full Displays all input/output plugs regardless of connection status Attribute Editor All Node-Based 3D Software architectures with a visual interface feature some kind of editor to customize node parameters. You may recognize this menu in Unity's UI: Unity Attribute Editor My Attribute Editor Users are able to select a specific plug in a node and access its data objects, parameters, and variable name Users can also click the Affects > button on the top right to view which downstream nodes are dependent on the selected plug Smart Default Layout Algorithm The previous examples have been relatively simple in scale and complexity A good visualizer should be scalable to handle the rendering demands of large node networks Question: How to draw the prettiest graph? First, let's define the constraints of an aesthetic graph Layer Assignment : There should be a clear hierarchy of nodes, where each node is assigned to a layer and each node is dependent only on nodes that reside in a preceding layer. This layout improves readability by introducing a uni-directional flow of information (left to right or top down) In Dependency Graph programming, it is crucial to see which nodes are further 'downstream' for debugging purposes Crossing Reduction : Ideally, there should be no crossing of the connections between nodes. If crossing elimination is impossible, then the nodes should be drawn in a way that minimizes crossings Best Approach The optimal solution to the Layered Graph Drawing problem is known as the Sugiyama Technique . I used the set of algorithms described in this paper: An Efficient Implementation of Sugiyama\u2019s Algorithm for Layered Graph Drawing (Eiglsperger et al. , 2005) NOTE: This algorithm only works for Directed Acyclic Graphs (DAG) , so the first step in implementation is to identify eliminate all existing cycles. Video Demo 3. Technical Specifications Tools and Technologies: Qt Design Studio: Version 4.0 Qt Framework: Version 6.6 Programming Language: C++, qml Platform Compatibility: Windows, macOS, Linux Ensure responsiveness across various screen sizes and resolutions User Interface Design Guidelines: Consistent UI/UX design adhering to best practices Accessibility considerations for differently-abled developers Explanation of the research paper on graph layout algorithms Integration of graph layout algorithms into the GUI Details on the implementation approach and its relevance to the project Expected impact on the GUI's usability and functionality 5. Development Phases Phase 1: Planning and Design Requirement gathering and analysis UI/UX wireframing and prototyping using Qt Design Studio Feedback collection and iterations Phase 2: Implementation Front-end development using Qt Design Studio Integration of backend functionalities (if applicable) Implementation of graph layout algorithms Continuous testing and debugging Phase 3: Quality Assurance and Testing Comprehensive testing across multiple platforms Usability testing with target users (developers) Bug fixing and performance optimization Phase 4: Deployment and Documentation Packaging the GUI for different platforms Preparation of user documentation and guidelines Release and deployment strategies 6. Risk Assessment and Mitigation Identification of potential risks: Compatibility issues with different operating systems Complexity in integrating version control systems User acceptance challenges due to UI/UX design Strategies for risk mitigation: Regular testing and compatibility checks Collaboration with version control system providers Continuous user feedback and iterative design improvements 7. Timeline and Milestones 9. Conclusion The Design Goals for the project were to make an intuitive, readable, and aesthetic interface for Dependency Graph programming. I accomplished the following: Functioning user interface with draggable, instantiable, and customizable nodes and connections C++ compatible Node Graph layout algorithm implemented This project will increase developer productivity by providing an easy-to-use interface that streamlines 3D development workflows Future considerations and potential enhancements: More plug and data types Complex objets 10. References and Appendices An Efficient Implementation of Sugiyama\u2019s Algorithm for Layered Graph Drawing (Eiglsperger et al. , 2005) Layered Graph Drawing (Wikipedia)","title":"Node Graph Visualizer"},{"location":"project3/#node-graph-visualizer","text":"","title":"Node Graph Visualizer"},{"location":"project3/#node-graph-visualizer_1","text":"","title":"Node Graph Visualizer"},{"location":"project3/#1-introduction","text":"The Node Graph Visualizer is a graphical user interface that visually displays relationships between nodes in a Dependency Graph software architecture The Dependency Graph (aka \"Node Graph\") structure is the industry standard for developing graphics applications The corresponding Visualizer is simply a user-friendly and code-free interface that can be used to manipulate nodes in real-time rendering applications . Maya , ZBrush , Houdini , Blender , Unity , Cinema 4D , and most other 3D Content Creation tools use a Dependency Graph for their codebase due its efficiency, flexibility, and compatibility with graphics applications. These softwares also include a code-free interface for technical users to create custom nodes and plug-ins. For example, Autodesk Maya's Hypershade was the blueprint for this visualizer project. Hypershade","title":"1. Introduction"},{"location":"project3/#2-goals-and-objectives","text":"The UI design goals were: Create an intuitive and aesthetic interface for developers Streamline workflows and improve productivity Enable easy integration with existing C++ codebase Implementation Objectives: Smart Default Layout Draggable Nodes Pin Objects Instantiate new nodes Draw Connections Delete Nodes/Connections Edit Plug Attributes View Plug Attributes","title":"2. Goals and Objectives"},{"location":"project3/#key-features","text":"","title":"Key Features"},{"location":"project3/#viewing-modes","text":"Simple Displays only one input plug and output plug Remaining plugs are hidden Connected Displays only the input/output plugs that are actively connected Full Displays all input/output plugs regardless of connection status","title":"Viewing Modes"},{"location":"project3/#attribute-editor","text":"All Node-Based 3D Software architectures with a visual interface feature some kind of editor to customize node parameters. You may recognize this menu in Unity's UI: Unity Attribute Editor My Attribute Editor Users are able to select a specific plug in a node and access its data objects, parameters, and variable name Users can also click the Affects > button on the top right to view which downstream nodes are dependent on the selected plug","title":"Attribute Editor"},{"location":"project3/#smart-default-layout-algorithm","text":"The previous examples have been relatively simple in scale and complexity A good visualizer should be scalable to handle the rendering demands of large node networks","title":"Smart Default Layout Algorithm"},{"location":"project3/#question-how-to-draw-the-prettiest-graph","text":"First, let's define the constraints of an aesthetic graph Layer Assignment : There should be a clear hierarchy of nodes, where each node is assigned to a layer and each node is dependent only on nodes that reside in a preceding layer. This layout improves readability by introducing a uni-directional flow of information (left to right or top down) In Dependency Graph programming, it is crucial to see which nodes are further 'downstream' for debugging purposes Crossing Reduction : Ideally, there should be no crossing of the connections between nodes. If crossing elimination is impossible, then the nodes should be drawn in a way that minimizes crossings","title":"Question: How to draw the prettiest graph?"},{"location":"project3/#best-approach","text":"The optimal solution to the Layered Graph Drawing problem is known as the Sugiyama Technique . I used the set of algorithms described in this paper: An Efficient Implementation of Sugiyama\u2019s Algorithm for Layered Graph Drawing (Eiglsperger et al. , 2005) NOTE: This algorithm only works for Directed Acyclic Graphs (DAG) , so the first step in implementation is to identify eliminate all existing cycles.","title":"Best Approach"},{"location":"project3/#video-demo","text":"","title":"Video Demo"},{"location":"project3/#3-technical-specifications","text":"","title":"3. Technical Specifications"},{"location":"project3/#tools-and-technologies","text":"Qt Design Studio: Version 4.0 Qt Framework: Version 6.6 Programming Language: C++, qml","title":"Tools and Technologies:"},{"location":"project3/#platform-compatibility","text":"Windows, macOS, Linux Ensure responsiveness across various screen sizes and resolutions","title":"Platform Compatibility:"},{"location":"project3/#user-interface-design-guidelines","text":"Consistent UI/UX design adhering to best practices Accessibility considerations for differently-abled developers Explanation of the research paper on graph layout algorithms Integration of graph layout algorithms into the GUI Details on the implementation approach and its relevance to the project Expected impact on the GUI's usability and functionality","title":"User Interface Design Guidelines:"},{"location":"project3/#5-development-phases","text":"","title":"5. Development Phases"},{"location":"project3/#phase-1-planning-and-design","text":"Requirement gathering and analysis UI/UX wireframing and prototyping using Qt Design Studio Feedback collection and iterations","title":"Phase 1: Planning and Design"},{"location":"project3/#phase-2-implementation","text":"Front-end development using Qt Design Studio Integration of backend functionalities (if applicable) Implementation of graph layout algorithms Continuous testing and debugging","title":"Phase 2: Implementation"},{"location":"project3/#phase-3-quality-assurance-and-testing","text":"Comprehensive testing across multiple platforms Usability testing with target users (developers) Bug fixing and performance optimization","title":"Phase 3: Quality Assurance and Testing"},{"location":"project3/#phase-4-deployment-and-documentation","text":"Packaging the GUI for different platforms Preparation of user documentation and guidelines Release and deployment strategies","title":"Phase 4: Deployment and Documentation"},{"location":"project3/#6-risk-assessment-and-mitigation","text":"Identification of potential risks: Compatibility issues with different operating systems Complexity in integrating version control systems User acceptance challenges due to UI/UX design Strategies for risk mitigation: Regular testing and compatibility checks Collaboration with version control system providers Continuous user feedback and iterative design improvements","title":"6. Risk Assessment and Mitigation"},{"location":"project3/#7-timeline-and-milestones","text":"","title":"7. Timeline and Milestones"},{"location":"project3/#9-conclusion","text":"The Design Goals for the project were to make an intuitive, readable, and aesthetic interface for Dependency Graph programming. I accomplished the following: Functioning user interface with draggable, instantiable, and customizable nodes and connections C++ compatible Node Graph layout algorithm implemented This project will increase developer productivity by providing an easy-to-use interface that streamlines 3D development workflows Future considerations and potential enhancements: More plug and data types Complex objets","title":"9. Conclusion"},{"location":"project3/#10-references-and-appendices","text":"An Efficient Implementation of Sugiyama\u2019s Algorithm for Layered Graph Drawing (Eiglsperger et al. , 2005) Layered Graph Drawing (Wikipedia)","title":"10. References and Appendices"},{"location":"project4/","text":"OpenGL Mesh Viewer OpenGL-based Mesh Viewer Implementation Objective The task involves creating an OpenGL-based mesh viewer capable of displaying multiple models simultaneously, each shaded using the Blinn-Phong illumination model. User interaction functionalities, such as mouse-driven model rotation and camera movement, are also implemented. The viewer is expected to load models, apply transformations for scaling and positioning, and render them alongside interactions and appropriate lighting effects. Components and Implementation Model Loading and Display The application loads models provided as command-line arguments and arranges them uniformly within a 2x2x2 cube centered at the origin. A new TriMesh class manages model loading (Load), bounding box calculation (GetBoundingBox), OpenGL buffer handling (DeleteGLBuffers, UpdateGLBuffers), and rendering (DrawGL). Shading and Lighting Implements the Blinn-Phong shading model for each model, computed within the fragment shader, resulting in smoother color transitions and improved specular highlights. Utilizes three point lights with varying positions and colors to illuminate the models, enhancing their visual appearance. User Interaction Model Rotation: Allows users to click and drag the mouse to rotate the models around the origin. The x and y changes in mouse coordinates determine the rotation matrices applied to the models. Camera Movement: Incorporates keyboard input (x and z keys) to adjust the camera's distance from the origin along the z-axis, preventing the camera from moving too close to the models. Reset Functionality: Pressing the spacebar resets both the model rotations and the camera position. Camera Setup Initially positions the camera at (0, 0, 2), looking at the origin along the -z direction, with an up-vector of (0, 1, 0), and a vertical field of view of 60 degrees. TriMesh Class Overview Constructors and Destructors Constructor: TriMesh(const std::string &name) : Initializes a TriMesh object with a specified name. Sets the name as \"TriMesh\" if no name is provided. Destructor: ~TriMesh() : Cleans up OpenGL buffers by calling DeleteGLBuffers() . Public Member Functions void TriMesh::GetBoundingBox(Vec3r &bmin, Vec3r &bmax) { // iterate over every vertex for (auto vit = vertices_begin(); vit != vertices_end(); ++vit){ Vec3r point = this->point(*vit); bmin[0] = std::min(bmin[0], point[0]); bmin[1] = std::min(bmin[1], point[1]); bmin[2] = std::min(bmin[2], point[2]); bmax[0] = std::max(bmax[0], point[0]); bmax[1] = std::max(bmax[1], point[1]); bmax[2] = std::max(bmax[2], point[2]); } } GetBoundingBox(Vec3r &bmin, Vec3r &bmax) : Computes the bounding box of the mesh by iterating over each vertex and updating the minimum and maximum coordinates. bool TriMesh::Load(const fs::path &filepath) { OpenMesh::IO::Options opts = OpenMesh::IO::Options::VertexTexCoord | OpenMesh::IO::Options::VertexNormal | OpenMesh::IO::Options::FaceNormal; // save name filepath_ = filepath; // request vertex texture coordinates request_vertex_texcoords2D(); // request vertex and face normals request_vertex_normals(); request_face_normals(); // read in mesh auto filename = filepath_.string(); spdlog::info (\"loading {}...\", filename); // check if the mesh file contained vertex (or face normals) normals // if not, compute normals if(!opts.check(OpenMesh::IO::Options::FaceNormal)) update_face_normals(); if(!opts.check(OpenMesh::IO::Options::VertexNormal)) update_vertex_normals(); if (has_halfedge_normals()) update_halfedge_normals(); return true; } Load(const fs::path &filepath) : Loads a mesh from a specified file path, reading the mesh data and computing normals if needed. bool TriMesh::ComputeFaceNormals() { auto FaceNormal = [&](TriMesh::FaceHandle fh) { auto heh = halfedge_handle(fh); Vec3r p0 = point(from_vertex_handle(heh)); Vec3r p1 = point(to_vertex_handle(heh)); auto vh2 = to_vertex_handle(next_halfedge_handle(heh)); Vec3r p2 = point(vh2); Vec3r face_normal = (p1 - p0).cross(p2 - p0); return face_normal.normalized(); }; if (!has_face_normals()) return false; for (auto fit = faces_begin(); fit != faces_end(); ++fit) set_normal(*fit, FaceNormal(*fit)); return true; } ComputeFaceNormals() : Computes face normals for the mesh. bool TriMesh::ComputeVertexNormals() { auto VertexNormal = [&](TriMesh::VertexHandle vh) { Vec3r vertex_normal{0, 0, 0}; for (auto vfit = vf_iter(vh); vfit.is_valid(); ++vfit) vertex_normal += normal(*vfit); vertex_normal.normalize(); return vertex_normal; }; if (!has_vertex_normals() || !has_face_normals()) return false; for (auto vit = vertices_begin(); vit != vertices_end(); ++vit) set_normal(*vit, VertexNormal(*vit)); return true; } ComputeVertexNormals() : Computes vertex normals for the mesh based on face normals. Internal Data Members OpenGL Buffer IDs: positions_normals_vbo_ , faces_ebo_ : IDs for vertex and element buffer objects used by OpenGL. Mesh Information: filepath_ , name_ : Filepath and name of the mesh. GL Buffer Status: gl_buffers_dirty_ : A flag indicating if OpenGL buffers need updating. External Dependencies External Libraries: spdlog , boost::filesystem . OpenGL Utilities: GLDrawData , GLShader . Main ( main.cc ) Overview Functions void GetViewAndProjectionMatrices(glm::mat4 &view_matrix, glm::mat4 &proj_matrix) { // compute aspect ratio float aspect = static_cast<float>(window_size_g[0]) / static_cast<float>(window_size_g[1]); view_matrix = glm::lookAt(glm::vec3(0, 0, camera_z_pos), glm::vec3(0, 0, -2), glm::vec3(0, 1, 0)); proj_matrix = glm::perspective(static_cast<float>(60.0 * kDEGtoRAD), aspect, 0.01f, 50.0f); } GetViewAndProjectionMatrices : Computes view and projection matrices based on window dimensions. Mat4r TransformMesh(int index, TriMesh::Ptr m) { // scale to fit inside unit cube Mat4r scale_xform{Mat4r::Identity()}; int i = 0; TriMesh::Ptr mesh = m; // find current mesh by index... Vec3r bmin, bmax; mesh->GetBoundingBox(bmin, bmax); Real xdim = bmax[0]-bmin[0]; Real ydim = bmax[1]-bmin[1]; Real zdim = bmax[2]-bmin[2]; // // scale so that the maximum dimension = 1; auto maxdim = std::max(xdim,ydim); maxdim = std::max(maxdim, zdim); Real scale = 2.0 / maxdim; scale /= i; scale_xform.diagonal() = Vec4r{scale, scale, scale, 1}; Vec3r center = scale*(bmin + 0.5*(bmax-bmin)); // midpoint of longest line double size = 2.0; // objects must fit within 2x2x2 box double xshift = index*(size/i) + size/(i*2) - size/2; // placement of object along x-axis Mat4r translate_xform{Mat4r::Identity()}; // Compute Transformation Matrices (RotateX, RotateY, Translate, Scale) // ... mesh_xform_g = translate_xform * rotate_y_xform * rotate_x_xform * scale_xform ; return (rotate_y_xform * rotate_x_xform * translate_xform * scale_xform ); // meshlist_xform_g.push_back(mesh_xform_g); } TransformMesh : Function to transform mesh based on user input Mat4r ResetMesh(int index, TriMesh::Ptr m) { int i = 0; TriMesh::Ptr mesh = m; // find current mesh by index ... delta_x = 0; delta_y = 0; Mat4r total_rotate_x_xform{Mat4r::Identity()}; // Compute rotation x matrix ... Mat4r total_rotate_y_xform{Mat4r::Identity()}; // Compute rotation y matrix ... Mat4r inv_rotate_x_xform = total_rotate_x_xform.inverse(); Mat4r inv_rotate_y_xform = total_rotate_y_xform.inverse(); return (inv_rotate_y_xform * inv_rotate_x_xform); } ResetMesh : Function to reset transformation on meshes based on previous changes made void UpdateSphere(Real glfw_time) { // scale 2x along x-axis Mat4r scale_xform{Mat4r::Identity()}; scale_xform.diagonal() = Vec4r{2, 1, 1, 1}; // rotate around y-axis based on current time Real rotation_speed = 90; // 90 degrees per second Mat4r rotate_y_xform{Mat4r::Identity()}; Real rotate_y_angle = glfw_time * rotation_speed; // Calculate Rotation Matrix // rotate around z-axis based on current time rotation_speed = 30; // 30 degrees per second Mat4r rotate_z_xform{Mat4r::Identity()}; Real rotate_z_angle = glfw_time * rotation_speed; // Calculate Rotation Matrix // compose sphere's transformation matrix sphere_xform_g = rotate_z_xform * rotate_y_xform * scale_xform; } UpdateSphere : Updates the transformation of a sphere based on time. Main Function The main function initializes the OpenGL window, sets up the VAO, creates objects (spheres and meshes), sets materials and lights, and runs the main rendering loop. The result is an interactive 3D rendering application where you can manipulate objects using keyboard inputs and mouse interactions. It supports rendering spheres and meshes with materials and lighting in an OpenGL context. The code structure includes functionality for handling different types of 3D objects, setting up materials, and handling user inputs to interact with the rendered scene. Technical Implementation Notes Shader Implementation Implements new vertex and fragment shaders to facilitate the Blinn-Phong shading model for enhanced visual quality and lighting effects. OpenGL Rendering Utilizes OpenGL functionality to manage and display meshes, leveraging vertex buffer objects for efficient rendering. User Interaction Handling Incorporates GLFW callbacks for mouse button and cursor position events to manage model rotation. Manages keyboard input to adjust the camera's distance from the models and reset functionalities. Result and Recommendations The completed assignment provides a functional OpenGL-based mesh viewer capable of loading, positioning, and shading multiple models using the Blinn-Phong illumination model. It offers interactive features for user-controlled model rotation and camera movement, ensuring a dynamic and immersive viewing experience. Recommendations for further improvements might include optimizations for rendering performance, additional shader effects, or extending the application's functionality to support more complex meshes or textures.","title":"OpenGL Mesh Viewer"},{"location":"project4/#opengl-mesh-viewer","text":"","title":"OpenGL Mesh Viewer"},{"location":"project4/#opengl-based-mesh-viewer-implementation","text":"","title":"OpenGL-based Mesh Viewer Implementation"},{"location":"project4/#objective","text":"The task involves creating an OpenGL-based mesh viewer capable of displaying multiple models simultaneously, each shaded using the Blinn-Phong illumination model. User interaction functionalities, such as mouse-driven model rotation and camera movement, are also implemented. The viewer is expected to load models, apply transformations for scaling and positioning, and render them alongside interactions and appropriate lighting effects.","title":"Objective"},{"location":"project4/#components-and-implementation","text":"Model Loading and Display The application loads models provided as command-line arguments and arranges them uniformly within a 2x2x2 cube centered at the origin. A new TriMesh class manages model loading (Load), bounding box calculation (GetBoundingBox), OpenGL buffer handling (DeleteGLBuffers, UpdateGLBuffers), and rendering (DrawGL). Shading and Lighting Implements the Blinn-Phong shading model for each model, computed within the fragment shader, resulting in smoother color transitions and improved specular highlights. Utilizes three point lights with varying positions and colors to illuminate the models, enhancing their visual appearance. User Interaction Model Rotation: Allows users to click and drag the mouse to rotate the models around the origin. The x and y changes in mouse coordinates determine the rotation matrices applied to the models. Camera Movement: Incorporates keyboard input (x and z keys) to adjust the camera's distance from the origin along the z-axis, preventing the camera from moving too close to the models. Reset Functionality: Pressing the spacebar resets both the model rotations and the camera position. Camera Setup Initially positions the camera at (0, 0, 2), looking at the origin along the -z direction, with an up-vector of (0, 1, 0), and a vertical field of view of 60 degrees.","title":"Components and Implementation"},{"location":"project4/#trimesh-class-overview","text":"","title":"TriMesh Class Overview"},{"location":"project4/#constructors-and-destructors","text":"Constructor: TriMesh(const std::string &name) : Initializes a TriMesh object with a specified name. Sets the name as \"TriMesh\" if no name is provided. Destructor: ~TriMesh() : Cleans up OpenGL buffers by calling DeleteGLBuffers() .","title":"Constructors and Destructors"},{"location":"project4/#public-member-functions","text":"void TriMesh::GetBoundingBox(Vec3r &bmin, Vec3r &bmax) { // iterate over every vertex for (auto vit = vertices_begin(); vit != vertices_end(); ++vit){ Vec3r point = this->point(*vit); bmin[0] = std::min(bmin[0], point[0]); bmin[1] = std::min(bmin[1], point[1]); bmin[2] = std::min(bmin[2], point[2]); bmax[0] = std::max(bmax[0], point[0]); bmax[1] = std::max(bmax[1], point[1]); bmax[2] = std::max(bmax[2], point[2]); } } GetBoundingBox(Vec3r &bmin, Vec3r &bmax) : Computes the bounding box of the mesh by iterating over each vertex and updating the minimum and maximum coordinates. bool TriMesh::Load(const fs::path &filepath) { OpenMesh::IO::Options opts = OpenMesh::IO::Options::VertexTexCoord | OpenMesh::IO::Options::VertexNormal | OpenMesh::IO::Options::FaceNormal; // save name filepath_ = filepath; // request vertex texture coordinates request_vertex_texcoords2D(); // request vertex and face normals request_vertex_normals(); request_face_normals(); // read in mesh auto filename = filepath_.string(); spdlog::info (\"loading {}...\", filename); // check if the mesh file contained vertex (or face normals) normals // if not, compute normals if(!opts.check(OpenMesh::IO::Options::FaceNormal)) update_face_normals(); if(!opts.check(OpenMesh::IO::Options::VertexNormal)) update_vertex_normals(); if (has_halfedge_normals()) update_halfedge_normals(); return true; } Load(const fs::path &filepath) : Loads a mesh from a specified file path, reading the mesh data and computing normals if needed. bool TriMesh::ComputeFaceNormals() { auto FaceNormal = [&](TriMesh::FaceHandle fh) { auto heh = halfedge_handle(fh); Vec3r p0 = point(from_vertex_handle(heh)); Vec3r p1 = point(to_vertex_handle(heh)); auto vh2 = to_vertex_handle(next_halfedge_handle(heh)); Vec3r p2 = point(vh2); Vec3r face_normal = (p1 - p0).cross(p2 - p0); return face_normal.normalized(); }; if (!has_face_normals()) return false; for (auto fit = faces_begin(); fit != faces_end(); ++fit) set_normal(*fit, FaceNormal(*fit)); return true; } ComputeFaceNormals() : Computes face normals for the mesh. bool TriMesh::ComputeVertexNormals() { auto VertexNormal = [&](TriMesh::VertexHandle vh) { Vec3r vertex_normal{0, 0, 0}; for (auto vfit = vf_iter(vh); vfit.is_valid(); ++vfit) vertex_normal += normal(*vfit); vertex_normal.normalize(); return vertex_normal; }; if (!has_vertex_normals() || !has_face_normals()) return false; for (auto vit = vertices_begin(); vit != vertices_end(); ++vit) set_normal(*vit, VertexNormal(*vit)); return true; } ComputeVertexNormals() : Computes vertex normals for the mesh based on face normals.","title":"Public Member Functions"},{"location":"project4/#internal-data-members","text":"OpenGL Buffer IDs: positions_normals_vbo_ , faces_ebo_ : IDs for vertex and element buffer objects used by OpenGL. Mesh Information: filepath_ , name_ : Filepath and name of the mesh. GL Buffer Status: gl_buffers_dirty_ : A flag indicating if OpenGL buffers need updating.","title":"Internal Data Members"},{"location":"project4/#external-dependencies","text":"External Libraries: spdlog , boost::filesystem . OpenGL Utilities: GLDrawData , GLShader .","title":"External Dependencies"},{"location":"project4/#main-maincc-overview","text":"","title":"Main (main.cc) Overview"},{"location":"project4/#functions","text":"void GetViewAndProjectionMatrices(glm::mat4 &view_matrix, glm::mat4 &proj_matrix) { // compute aspect ratio float aspect = static_cast<float>(window_size_g[0]) / static_cast<float>(window_size_g[1]); view_matrix = glm::lookAt(glm::vec3(0, 0, camera_z_pos), glm::vec3(0, 0, -2), glm::vec3(0, 1, 0)); proj_matrix = glm::perspective(static_cast<float>(60.0 * kDEGtoRAD), aspect, 0.01f, 50.0f); } GetViewAndProjectionMatrices : Computes view and projection matrices based on window dimensions. Mat4r TransformMesh(int index, TriMesh::Ptr m) { // scale to fit inside unit cube Mat4r scale_xform{Mat4r::Identity()}; int i = 0; TriMesh::Ptr mesh = m; // find current mesh by index... Vec3r bmin, bmax; mesh->GetBoundingBox(bmin, bmax); Real xdim = bmax[0]-bmin[0]; Real ydim = bmax[1]-bmin[1]; Real zdim = bmax[2]-bmin[2]; // // scale so that the maximum dimension = 1; auto maxdim = std::max(xdim,ydim); maxdim = std::max(maxdim, zdim); Real scale = 2.0 / maxdim; scale /= i; scale_xform.diagonal() = Vec4r{scale, scale, scale, 1}; Vec3r center = scale*(bmin + 0.5*(bmax-bmin)); // midpoint of longest line double size = 2.0; // objects must fit within 2x2x2 box double xshift = index*(size/i) + size/(i*2) - size/2; // placement of object along x-axis Mat4r translate_xform{Mat4r::Identity()}; // Compute Transformation Matrices (RotateX, RotateY, Translate, Scale) // ... mesh_xform_g = translate_xform * rotate_y_xform * rotate_x_xform * scale_xform ; return (rotate_y_xform * rotate_x_xform * translate_xform * scale_xform ); // meshlist_xform_g.push_back(mesh_xform_g); } TransformMesh : Function to transform mesh based on user input Mat4r ResetMesh(int index, TriMesh::Ptr m) { int i = 0; TriMesh::Ptr mesh = m; // find current mesh by index ... delta_x = 0; delta_y = 0; Mat4r total_rotate_x_xform{Mat4r::Identity()}; // Compute rotation x matrix ... Mat4r total_rotate_y_xform{Mat4r::Identity()}; // Compute rotation y matrix ... Mat4r inv_rotate_x_xform = total_rotate_x_xform.inverse(); Mat4r inv_rotate_y_xform = total_rotate_y_xform.inverse(); return (inv_rotate_y_xform * inv_rotate_x_xform); } ResetMesh : Function to reset transformation on meshes based on previous changes made void UpdateSphere(Real glfw_time) { // scale 2x along x-axis Mat4r scale_xform{Mat4r::Identity()}; scale_xform.diagonal() = Vec4r{2, 1, 1, 1}; // rotate around y-axis based on current time Real rotation_speed = 90; // 90 degrees per second Mat4r rotate_y_xform{Mat4r::Identity()}; Real rotate_y_angle = glfw_time * rotation_speed; // Calculate Rotation Matrix // rotate around z-axis based on current time rotation_speed = 30; // 30 degrees per second Mat4r rotate_z_xform{Mat4r::Identity()}; Real rotate_z_angle = glfw_time * rotation_speed; // Calculate Rotation Matrix // compose sphere's transformation matrix sphere_xform_g = rotate_z_xform * rotate_y_xform * scale_xform; } UpdateSphere : Updates the transformation of a sphere based on time.","title":"Functions"},{"location":"project4/#main-function","text":"The main function initializes the OpenGL window, sets up the VAO, creates objects (spheres and meshes), sets materials and lights, and runs the main rendering loop. The result is an interactive 3D rendering application where you can manipulate objects using keyboard inputs and mouse interactions. It supports rendering spheres and meshes with materials and lighting in an OpenGL context. The code structure includes functionality for handling different types of 3D objects, setting up materials, and handling user inputs to interact with the rendered scene.","title":"Main Function"},{"location":"project4/#technical-implementation-notes","text":"Shader Implementation Implements new vertex and fragment shaders to facilitate the Blinn-Phong shading model for enhanced visual quality and lighting effects. OpenGL Rendering Utilizes OpenGL functionality to manage and display meshes, leveraging vertex buffer objects for efficient rendering. User Interaction Handling Incorporates GLFW callbacks for mouse button and cursor position events to manage model rotation. Manages keyboard input to adjust the camera's distance from the models and reset functionalities.","title":"Technical Implementation Notes"},{"location":"project4/#result-and-recommendations","text":"The completed assignment provides a functional OpenGL-based mesh viewer capable of loading, positioning, and shading multiple models using the Blinn-Phong illumination model. It offers interactive features for user-controlled model rotation and camera movement, ensuring a dynamic and immersive viewing experience. Recommendations for further improvements might include optimizations for rendering performance, additional shader effects, or extending the application's functionality to support more complex meshes or textures.","title":"Result and Recommendations"},{"location":"project5/","text":"C++ Ray Tracer Part 1: Antialiasing Implementation Updates: Modified main.cc in src/rtbasic to incorporate the -a command-line argument for defining the number of samples per pixel. Introduced RayTracer::SetNumSamplesPerPixel() and necessary data members to control the samples per pixel for antialiasing. Altered RayTracer::Render() to generate multiple random rays through each pixel for antialiasing, enhancing image quality. Part 2: TriMesh TriMesh Functionality: Implemented TriMesh functionalities ( Hit , RayFaceHit , Save , Load , GetBoundingBox , ComputeFaceNormals , ComputeVertexNormals ) in trimesh.cc . Enhanced scene parser ( raytra_parser.cc ) to load triangle meshes using the w command. Generated and verified rendering for sphere_mesh.scn and suzanne.scn . Mesh Generation: Implemented BuildWatertightSphere() and BuildSphereWithTexCoords() in main.cc of mesh_gen . BuildSphereWithTexCoords() generates a non-watertight sphere mesh with vertex texture coordinates mapped from spherical coordinates. BuildWatertightSphere() creates a watertight sphere mesh without boundary vertices, edges, or faces. Part 3: Texture Mapping FaceGeoUV Class: Implemented FaceGeoUV in face_geouv.h and face_geouv.cc , along with necessary modifications in HitRecord and Surface::Hit() functions for different surface types. Texture Handling: Implemented ImageTexture::Value() in image_texture.cc to evaluate textures at specific locations. Modified PhongMaterial to support diffuse textures, allowing evaluation of textures at hit points. Scene File Parser Update: Updated the scene file parser to support textured Phong materials using image textures. Part 4: Bounding Volume Hierarchy (BVH) BVHNode Implementation: - Completed BVHNode::BuildBVH() and BVHNode::Hit() functionalities in bvh_node.cc . Introduced a scene-level BVH in main.cc of rtbasic for efficient rendering. Mesh-level BVH: - Implemented TriMesh::BuildBVH() to construct a BVH of mesh triangle faces for optimized intersection tests. - Created a new BVHTriMeshFace class to represent individual TriMesh faces for BVH construction. Final Renders: Rendered scenes such as jug_triangle_soup.scn , sphere_mesh_textured.scn , jug.scn , and dandelion.scn to demonstrate improved rendering with added functionalities. The assignment includes significant enhancements to ray tracing capabilities, mesh handling, texture mapping, and BVH implementation. The additions allow for improved image quality, efficient mesh representation, and texture-driven material definitions in scene rendering.","title":"C++ Ray Tracer"},{"location":"project5/#c-ray-tracer","text":"","title":"C++ Ray Tracer"},{"location":"project5/#part-1-antialiasing","text":"","title":"Part 1: Antialiasing"},{"location":"project5/#implementation-updates","text":"Modified main.cc in src/rtbasic to incorporate the -a command-line argument for defining the number of samples per pixel. Introduced RayTracer::SetNumSamplesPerPixel() and necessary data members to control the samples per pixel for antialiasing. Altered RayTracer::Render() to generate multiple random rays through each pixel for antialiasing, enhancing image quality.","title":"Implementation Updates:"},{"location":"project5/#part-2-trimesh","text":"","title":"Part 2: TriMesh"},{"location":"project5/#trimesh-functionality","text":"Implemented TriMesh functionalities ( Hit , RayFaceHit , Save , Load , GetBoundingBox , ComputeFaceNormals , ComputeVertexNormals ) in trimesh.cc . Enhanced scene parser ( raytra_parser.cc ) to load triangle meshes using the w command. Generated and verified rendering for sphere_mesh.scn and suzanne.scn .","title":"TriMesh Functionality:"},{"location":"project5/#mesh-generation","text":"Implemented BuildWatertightSphere() and BuildSphereWithTexCoords() in main.cc of mesh_gen . BuildSphereWithTexCoords() generates a non-watertight sphere mesh with vertex texture coordinates mapped from spherical coordinates. BuildWatertightSphere() creates a watertight sphere mesh without boundary vertices, edges, or faces.","title":"Mesh Generation:"},{"location":"project5/#part-3-texture-mapping","text":"","title":"Part 3: Texture Mapping"},{"location":"project5/#facegeouv-class","text":"Implemented FaceGeoUV in face_geouv.h and face_geouv.cc , along with necessary modifications in HitRecord and Surface::Hit() functions for different surface types.","title":"FaceGeoUV Class:"},{"location":"project5/#texture-handling","text":"Implemented ImageTexture::Value() in image_texture.cc to evaluate textures at specific locations. Modified PhongMaterial to support diffuse textures, allowing evaluation of textures at hit points.","title":"Texture Handling:"},{"location":"project5/#scene-file-parser-update","text":"Updated the scene file parser to support textured Phong materials using image textures.","title":"Scene File Parser Update:"},{"location":"project5/#part-4-bounding-volume-hierarchy-bvh","text":"","title":"Part 4: Bounding Volume Hierarchy (BVH)"},{"location":"project5/#bvhnode-implementation","text":"- Completed BVHNode::BuildBVH() and BVHNode::Hit() functionalities in bvh_node.cc . Introduced a scene-level BVH in main.cc of rtbasic for efficient rendering.","title":"BVHNode Implementation:"},{"location":"project5/#mesh-level-bvh","text":"- Implemented TriMesh::BuildBVH() to construct a BVH of mesh triangle faces for optimized intersection tests. - Created a new BVHTriMeshFace class to represent individual TriMesh faces for BVH construction.","title":"Mesh-level BVH:"},{"location":"project5/#final-renders","text":"Rendered scenes such as jug_triangle_soup.scn , sphere_mesh_textured.scn , jug.scn , and dandelion.scn to demonstrate improved rendering with added functionalities. The assignment includes significant enhancements to ray tracing capabilities, mesh handling, texture mapping, and BVH implementation. The additions allow for improved image quality, efficient mesh representation, and texture-driven material definitions in scene rendering.","title":"Final Renders:"},{"location":"project6/","text":"VR Music Video: THUNDR (liKo) How to Play 1. Open the Web Application (Desktop or Mobile): 2. Press the Play button to launch the experience 3. Tap the ground to place thunder clouds in the sky overhead 4. Press the Thundercloud button to create a thunderstorm Animation Rig Overview The animationRigComponent is responsible for managing animations within a Three.js environment. Here's a technical breakdown of its functionalities: Constants const LoopMode = { once: THREE.LoopOnce, repeat: THREE.LoopRepeat, pingpong: THREE.LoopPingPong, } LoopMode : Defines different looping modes ( once , repeat , pingpong ) for animations using constants from the Three.js library. animationRigComponent Object Initialization ( init() ) init() { this.model = null, this.remoteModel = null, this.mixer = null, this.activeActions = [] let {remoteId} = this.data remoteId = remoteId.charAt(0) === '#' ? remoteId.slice(1) : remoteId const remoteEl = document.getElementById(remoteId) remoteEl || console.error('ramx: Remote entity not found. Pass the ID of the entity, not the model.'), this.model = this.el.getObject3D('mesh'), this.remoteModel = remoteEl.getObject3D('mesh') }, Fetches and verifies required elements ( model and remoteModel ) for animations. Listens for the loaded event of the required elements and initializes when both are available. load() Function load() { const {el} = this this.model.animations = [...this.remoteModel.animations], this.mixer = new THREE.AnimationMixer(this.model), this.mixer.addEventListener('loop', (e) => { el.emit('animation-loop', { action: e.action, loopDelta: e.loopDelta, }) }), this.mixer.addEventListener('finished', (e) => { el.emit('animation-finished', { action: e.action, direction: e.direction, }) }), this.data.clip && this.update({}) }, Loads animations onto the model from the remoteModel . Sets up a mixer to manage animations and emits events on loop and finish of animations. update(prevData) Function update(prevData) { if (!prevData) return const {data} = this const changes = AFRAME.utils.diff(data, prevData) if ('clip' in changes) { return this.stopAction(), void (data.clip && this.playAction()) } this.activeActions.forEach((action) => { 'duration' in changes && data.duration && action.setDuration(data.duration), 'clampWhenFinished' in changes && (action.clampWhenFinished = data.clampWhenFinished), ('loop' in changes || 'repetitions' in changes) && action.setLoop(LoopMode[data.loop], data.repetitions), 'timeScale' in changes && action.setEffectiveTimeScale(data.timeScale) }) } Monitors changes in component data and updates animations accordingly. Modifies active animation actions based on changes in data properties. stopAction() Function stopAction() { const {data} = this for (let i = 0; i < this.activeActions.length; i++) data.crossFadeDuration ? this.activeActions[i].fadeOut(data.crossFadeDuration) : this.activeActions[i].stop() this.activeActions = [] } Stops ongoing animation actions based on the updated data. playAction() Function playAction() { if (!this.mixer) return const {model} = this const {data} = this const clips = model.animations || (model.geometry || {}).animations || [] if (!clips.length) return const re = wildcardToRegExp(data.clip) for (let clip, i = 0; clip = clips[i]; i++) { if (clip.name.match(re)) { const action = this.mixer.clipAction(clip, model) action.enabled = !0, action.clampWhenFinished = data.clampWhenFinished, data.duration && action.setDuration(data.duration), data.timeScale !== 1 && action.setEffectiveTimeScale(data.timeScale), action.setLoop(LoopMode[data.loop], data.repetitions).fadeIn(data.crossFadeDuration).play(), this.activeActions.push(action) } } } Plays animations based on the provided clip pattern. Selects animations from available clips based on a wildcard match and configures their properties before playing. tick(t, dt) Function tick(t, dt) { this.mixer && !Number.isNaN(dt) && this.mixer.update(dt / 1e3) } Handles the Three.js animation loop by updating the mixer with the elapsed time ( dt ) if the mixer exists and dt is a valid number. Export Exports the animationRigComponent for use in the larger codebase. This component enables dynamic loading and management of animations onto 3D models within a Three.js scene. It allows for flexible control over animation playback, durations, looping, and time scaling, responding to changes in component data.","title":"VR Music Video"},{"location":"project6/#vr-music-video-thundr-liko","text":"","title":"VR Music Video: THUNDR (liKo)"},{"location":"project6/#how-to-play","text":"1. Open the Web Application (Desktop or Mobile): 2. Press the Play button to launch the experience 3. Tap the ground to place thunder clouds in the sky overhead 4. Press the Thundercloud button to create a thunderstorm","title":"How to Play"},{"location":"project6/#animation-rig-overview","text":"The animationRigComponent is responsible for managing animations within a Three.js environment. Here's a technical breakdown of its functionalities:","title":"Animation Rig Overview"},{"location":"project6/#constants","text":"const LoopMode = { once: THREE.LoopOnce, repeat: THREE.LoopRepeat, pingpong: THREE.LoopPingPong, } LoopMode : Defines different looping modes ( once , repeat , pingpong ) for animations using constants from the Three.js library.","title":"Constants"},{"location":"project6/#animationrigcomponent-object","text":"","title":"animationRigComponent Object"},{"location":"project6/#initialization-init","text":"init() { this.model = null, this.remoteModel = null, this.mixer = null, this.activeActions = [] let {remoteId} = this.data remoteId = remoteId.charAt(0) === '#' ? remoteId.slice(1) : remoteId const remoteEl = document.getElementById(remoteId) remoteEl || console.error('ramx: Remote entity not found. Pass the ID of the entity, not the model.'), this.model = this.el.getObject3D('mesh'), this.remoteModel = remoteEl.getObject3D('mesh') }, Fetches and verifies required elements ( model and remoteModel ) for animations. Listens for the loaded event of the required elements and initializes when both are available.","title":"Initialization (init())"},{"location":"project6/#load-function","text":"load() { const {el} = this this.model.animations = [...this.remoteModel.animations], this.mixer = new THREE.AnimationMixer(this.model), this.mixer.addEventListener('loop', (e) => { el.emit('animation-loop', { action: e.action, loopDelta: e.loopDelta, }) }), this.mixer.addEventListener('finished', (e) => { el.emit('animation-finished', { action: e.action, direction: e.direction, }) }), this.data.clip && this.update({}) }, Loads animations onto the model from the remoteModel . Sets up a mixer to manage animations and emits events on loop and finish of animations.","title":"load() Function"},{"location":"project6/#updateprevdata-function","text":"update(prevData) { if (!prevData) return const {data} = this const changes = AFRAME.utils.diff(data, prevData) if ('clip' in changes) { return this.stopAction(), void (data.clip && this.playAction()) } this.activeActions.forEach((action) => { 'duration' in changes && data.duration && action.setDuration(data.duration), 'clampWhenFinished' in changes && (action.clampWhenFinished = data.clampWhenFinished), ('loop' in changes || 'repetitions' in changes) && action.setLoop(LoopMode[data.loop], data.repetitions), 'timeScale' in changes && action.setEffectiveTimeScale(data.timeScale) }) } Monitors changes in component data and updates animations accordingly. Modifies active animation actions based on changes in data properties.","title":"update(prevData) Function"},{"location":"project6/#stopaction-function","text":"stopAction() { const {data} = this for (let i = 0; i < this.activeActions.length; i++) data.crossFadeDuration ? this.activeActions[i].fadeOut(data.crossFadeDuration) : this.activeActions[i].stop() this.activeActions = [] } Stops ongoing animation actions based on the updated data.","title":"stopAction() Function"},{"location":"project6/#playaction-function","text":"playAction() { if (!this.mixer) return const {model} = this const {data} = this const clips = model.animations || (model.geometry || {}).animations || [] if (!clips.length) return const re = wildcardToRegExp(data.clip) for (let clip, i = 0; clip = clips[i]; i++) { if (clip.name.match(re)) { const action = this.mixer.clipAction(clip, model) action.enabled = !0, action.clampWhenFinished = data.clampWhenFinished, data.duration && action.setDuration(data.duration), data.timeScale !== 1 && action.setEffectiveTimeScale(data.timeScale), action.setLoop(LoopMode[data.loop], data.repetitions).fadeIn(data.crossFadeDuration).play(), this.activeActions.push(action) } } } Plays animations based on the provided clip pattern. Selects animations from available clips based on a wildcard match and configures their properties before playing.","title":"playAction() Function"},{"location":"project6/#tickt-dt-function","text":"tick(t, dt) { this.mixer && !Number.isNaN(dt) && this.mixer.update(dt / 1e3) } Handles the Three.js animation loop by updating the mixer with the elapsed time ( dt ) if the mixer exists and dt is a valid number.","title":"tick(t, dt) Function"},{"location":"project6/#export","text":"Exports the animationRigComponent for use in the larger codebase. This component enables dynamic loading and management of animations onto 3D models within a Three.js scene. It allows for flexible control over animation playback, durations, looping, and time scaling, responding to changes in component data.","title":"Export"},{"location":"project7/","text":"Python Ray Tracer (Blender Plugin) Sample Output (Cornell Box Scene) Overview Project Name: simpleRT_plugin.py Purpose: Blender add-on for a minimal ray tracing engine designed for CS148 HW5. Features Implemented: - Area light support - Sampling techniques - Indirect illumination (global illumination) Plugin Information Name: simple_ray_tracer Description: Simple Ray Tracer for CS148 Author: CS148 Version: 0.0.2023 Blender Version Compatibility: 3.5.1 Category: Render Wiki URL: Stanford CS148 Course Features & Implementation Details 1. Ray Casting Uses Blender\u2019s built-in scene.ray_cast() function. Determines whether a ray intersects with objects in the scene. Returns hit location, normal, and object data. 2. Sampling & Hemisphere Sampling Implements Monte Carlo sampling with a hemisphere function. Generates uniformly distributed directions for indirect lighting calculations. Uses random values for spherical coordinates conversion. Code Snippet: def sample_hemisphere(): r1 = np.random.rand() r2 = np.random.rand() cos_theta = r1 sin_theta = np.sqrt(1 - cos_theta ** 2) phi = r2 * 2 * np.pi x = sin_theta * np.cos(phi) y = sin_theta * np.sin(phi) z = cos_theta return np.array([x, y, z]) 3. Transforming Local to World Space Converts sampled ray directions from local hemisphere space to world space. Uses a transformation matrix formed by the normal and two perpendicular basis vectors. Code Snippet: def transform_to_world_space(ray_direction, x, y, n): transform = np.array([x, y, n]).T return transform @ ray_direction 5. Area Light Implementation Adds support for area lights by sampling points on the light source. Adjusts intensity based on normal alignment (tilt factor). Code Snippet: if light.data.type == 'AREA': r = sqrt(np.random.rand()) theta = 2 * pi * np.random.rand() emit_loc_local = Vector((r * np.cos(theta), r * np.sin(theta), 0)) emit_loc_world = light.matrix_world @ emit_loc_local light_loc = emit_loc_world 6. Reflection and Transmission Implements reflectivity based on Fresnel equations. Handles transparency and refraction based on material index of refraction (IOR). Code Snippet: reflectivity = mat.mirror_reflectivity if mat.use_fresnel: n2 = mat.ior r0 = ((1 - n2) / (1 + n2)) ** 2 reflectivity = r0 + (1 - r0) * ((1 + ray_dir.dot(hit_norm)) ** 5) 7. Low-Discrepancy Sampling (Van der Corput Sequence) Used for anti-aliasing in rendering. Generates quasi-random offsets for pixel sampling. Code Snippet: def corput(n, base=2): q, denom = 0, 1 while n: denom *= base n, remainder = divmod(n, base) q += remainder / denom return q - 0.5 8. Rendering Pipeline Iterates through image pixels and samples per pixel. Uses recursive tracing for depth-based illumination effects. Implements a progress update system for rendering status. Code Snippet: for y in RT_render_scene(scene, width, height, depth, samples, buf): status = f\"pass {y//height+1}/{samples} | Remaining {timedelta(seconds=remain)}\" self.update_stats(\"\", status) print(status, end=\"\\r\") Performance Considerations Optimizations: Uses numpy for efficient vector operations. Implements stratified sampling for better noise reduction. Caches light calculations to reduce redundant operations. Potential Improvements: Implement acceleration structures (BVH or KD-Tree). Improve anti-aliasing with more advanced sampling strategies. Add support for additional shading models. Conclusion This Blender plugin extends the simpleRT engine with features such as area lighting, global illumination, and improved sampling techniques. The implementation follows standard ray tracing methodologies while leveraging Blender's built-in API for scene querying and rendering integration. Future improvements could focus on performance optimizations and additional rendering effects.","title":"Python Ray Tracer (Blender Plugin)"},{"location":"project7/#python-ray-tracer-blender-plugin","text":"","title":"Python Ray Tracer (Blender Plugin)"},{"location":"project7/#sample-output-cornell-box-scene","text":"","title":"Sample Output (Cornell Box Scene)"},{"location":"project7/#overview","text":"Project Name: simpleRT_plugin.py Purpose: Blender add-on for a minimal ray tracing engine designed for CS148 HW5. Features Implemented: - Area light support - Sampling techniques - Indirect illumination (global illumination)","title":"Overview"},{"location":"project7/#plugin-information","text":"Name: simple_ray_tracer Description: Simple Ray Tracer for CS148 Author: CS148 Version: 0.0.2023 Blender Version Compatibility: 3.5.1 Category: Render Wiki URL: Stanford CS148 Course","title":"Plugin Information"},{"location":"project7/#features-implementation-details","text":"","title":"Features &amp; Implementation Details"},{"location":"project7/#1-ray-casting","text":"Uses Blender\u2019s built-in scene.ray_cast() function. Determines whether a ray intersects with objects in the scene. Returns hit location, normal, and object data.","title":"1. Ray Casting"},{"location":"project7/#2-sampling-hemisphere-sampling","text":"Implements Monte Carlo sampling with a hemisphere function. Generates uniformly distributed directions for indirect lighting calculations. Uses random values for spherical coordinates conversion. Code Snippet: def sample_hemisphere(): r1 = np.random.rand() r2 = np.random.rand() cos_theta = r1 sin_theta = np.sqrt(1 - cos_theta ** 2) phi = r2 * 2 * np.pi x = sin_theta * np.cos(phi) y = sin_theta * np.sin(phi) z = cos_theta return np.array([x, y, z])","title":"2. Sampling &amp; Hemisphere Sampling"},{"location":"project7/#3-transforming-local-to-world-space","text":"Converts sampled ray directions from local hemisphere space to world space. Uses a transformation matrix formed by the normal and two perpendicular basis vectors. Code Snippet: def transform_to_world_space(ray_direction, x, y, n): transform = np.array([x, y, n]).T return transform @ ray_direction","title":"3. Transforming Local to World Space"},{"location":"project7/#5-area-light-implementation","text":"Adds support for area lights by sampling points on the light source. Adjusts intensity based on normal alignment (tilt factor). Code Snippet: if light.data.type == 'AREA': r = sqrt(np.random.rand()) theta = 2 * pi * np.random.rand() emit_loc_local = Vector((r * np.cos(theta), r * np.sin(theta), 0)) emit_loc_world = light.matrix_world @ emit_loc_local light_loc = emit_loc_world","title":"5. Area Light Implementation"},{"location":"project7/#6-reflection-and-transmission","text":"Implements reflectivity based on Fresnel equations. Handles transparency and refraction based on material index of refraction (IOR). Code Snippet: reflectivity = mat.mirror_reflectivity if mat.use_fresnel: n2 = mat.ior r0 = ((1 - n2) / (1 + n2)) ** 2 reflectivity = r0 + (1 - r0) * ((1 + ray_dir.dot(hit_norm)) ** 5)","title":"6. Reflection and Transmission"},{"location":"project7/#7-low-discrepancy-sampling-van-der-corput-sequence","text":"Used for anti-aliasing in rendering. Generates quasi-random offsets for pixel sampling. Code Snippet: def corput(n, base=2): q, denom = 0, 1 while n: denom *= base n, remainder = divmod(n, base) q += remainder / denom return q - 0.5","title":"7. Low-Discrepancy Sampling (Van der Corput Sequence)"},{"location":"project7/#8-rendering-pipeline","text":"Iterates through image pixels and samples per pixel. Uses recursive tracing for depth-based illumination effects. Implements a progress update system for rendering status. Code Snippet: for y in RT_render_scene(scene, width, height, depth, samples, buf): status = f\"pass {y//height+1}/{samples} | Remaining {timedelta(seconds=remain)}\" self.update_stats(\"\", status) print(status, end=\"\\r\")","title":"8. Rendering Pipeline"},{"location":"project7/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"project7/#optimizations","text":"Uses numpy for efficient vector operations. Implements stratified sampling for better noise reduction. Caches light calculations to reduce redundant operations.","title":"Optimizations:"},{"location":"project7/#potential-improvements","text":"Implement acceleration structures (BVH or KD-Tree). Improve anti-aliasing with more advanced sampling strategies. Add support for additional shading models.","title":"Potential Improvements:"},{"location":"project7/#conclusion","text":"This Blender plugin extends the simpleRT engine with features such as area lighting, global illumination, and improved sampling techniques. The implementation follows standard ray tracing methodologies while leveraging Blender's built-in API for scene querying and rendering integration. Future improvements could focus on performance optimizations and additional rendering effects.","title":"Conclusion"},{"location":"project8/","text":"Final Render for Stanford CS148 Blender Recreation of my iMac Succulent","title":"Blender Projects"},{"location":"project8/#final-render-for-stanford-cs148","text":"","title":"Final Render for Stanford CS148"},{"location":"project8/#blender-recreation-of-my-imac","text":"","title":"Blender Recreation of my iMac"},{"location":"project8/#succulent","text":"","title":"Succulent"}]}